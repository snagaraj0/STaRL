{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4834cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "TEMP = 0.7\n",
    "MAX_NEW_TOKENS = 1024\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_SIZE = 5000\n",
    "VAL_SIZE = 500\n",
    "LOG_EVERY = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOG_JSON_DIR = \"batch_logs\"\n",
    "os.makedirs(LOG_JSON_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f168693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\" #Qwen/Qwen3-1.7B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8a7efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "Explanation: A bald eagle is found in North America, and the answer choices are all of this. The correct answer is that a bald eagle is safe in its habitat. Bald eagles live in forests, marshes, lakes, rivers, and wetlands. The correct answer is D.\n"
     ]
    }
   ],
   "source": [
    "# Test base model generation quality \n",
    "model.eval()\n",
    "sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfdb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQAParser:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. \n",
    "\n",
    "For each question:\n",
    "1. Identify what the question is asking for\n",
    "2. Evaluate each choice against the question's requirements\n",
    "3. Select the choice that best fits\n",
    "\n",
    "Examples:\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen?\n",
    "Answer Choices: (a) shirt pocket (b) calligrapher's hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The question asks for something that absorbs ink. A blotter is specifically designed to absorb excess ink from writing instruments. Therefore, the answer is blotter (e).\n",
    "\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet (e) desk\n",
    "A: The question asks for entertainment equipment that needs cable service. Televisions can receive cable TV signals for entertainment programming. Therefore, the answer is television (c).\n",
    "\n",
    "Always format your response as:\n",
    "\"<Clear reasoning in 1-2 sentences>. Therefore, the answer is <answer text> (<letter>).\"\n",
    "\n",
    "Choose the most reasonable answer if uncertain.\"\"\"\n",
    "    def format_question(self, question_data):\n",
    "        q = question_data['question']\n",
    "        choices = \"\".join(\n",
    "            f\"({lbl.lower()}) {txt}\\n\"\n",
    "            for lbl, txt in zip(\n",
    "                question_data['choices']['label'], question_data['choices']['text']\n",
    "            )\n",
    "        )\n",
    "        return f\"Q: {q}\\nAnswer Choices:\\n{choices.strip()}\\nA: \"\n",
    "\n",
    "    def format_prompt(self, question_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\",  \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\",    \"content\": self.format_question(question_data)}\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        ), messages[-1]['content']\n",
    "\n",
    "    def parse_llm_output(self, generated_text):\n",
    "        rationale = generated_text.removeprefix(\"</think>\").strip()\n",
    "        matches = re.findall(r\"\\(([a-e])\\)\", generated_text, re.IGNORECASE)\n",
    "        letter = matches[-1].lower() if matches else None\n",
    "        return rationale, letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1bc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_no_grad(model, batch_prompt_ids, max_new_tokens=MAX_NEW_TOKENS, temp=TEMP):\n",
    "    # `batch_prompt_ids` is shape (B, T)\n",
    "    seq = model.generate(\n",
    "        batch_prompt_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    # We return only the newly generated portion\n",
    "    return seq[:, batch_prompt_ids.size(1):]  # shape (B, new_T)\n",
    "\n",
    "def compute_logprobs(model, batch_prompt_ids, batch_gen_ids, temp=TEMP):\n",
    "    \"\"\"\n",
    "    # batch_prompt_ids: (B, T)\n",
    "    # batch_gen_ids:    (B, new_T)\n",
    "    full_ids = torch.cat([batch_prompt_ids, batch_gen_ids], dim=1)  # (B, T+new_T)\n",
    "    full_logits = model(full_ids).logits / temp  # (B, T+new_T, V)\n",
    "    full_logprobs = F.log_softmax(full_logits, dim=-1) # (B, T+new_T, V)\n",
    "\n",
    "    # log-probs of actual tokens (i.e., log P(token_t | token_<t))\n",
    "    token_logprobs = full_logprobs[:, :-1, :].gather(\n",
    "        2, full_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T_total - 1)\n",
    "\n",
    "    prompt_lens = (batch_prompt_ids != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1) # (B,)\n",
    "\n",
    "    logprobs = []\n",
    "    for i in range(batch_prompt_ids.size(0)):\n",
    "        start = int(prompt_lens[i].item()) - 1\n",
    "        gen_len = int(gen_lens[i].item())\n",
    "        logprobs.append(token_logprobs[i, start : start + gen_len].sum())\n",
    "\n",
    "    return torch.stack(logprobs, dim=0)  # (B,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate inputs\n",
    "    full_ids = torch.cat([batch_prompt_ids, batch_gen_ids], dim=1)  # (B, T+new_T)\n",
    "\n",
    "    # Forward pass\n",
    "    full_logits = model(full_ids).logits / temp  # (B, T+new_T, V)\n",
    "    full_logprobs = F.log_softmax(full_logits, dim=-1)  # (B, T+new_T, V)\n",
    "\n",
    "    # Get logprobs of actual tokens (predict token_t at position t)\n",
    "    token_logprobs = full_logprobs[:, :-1, :].gather(\n",
    "        2, full_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T+new_T - 1)\n",
    "\n",
    "    # Lengths\n",
    "    prompt_lens = (batch_prompt_ids != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1)        # (B,)\n",
    "\n",
    "    # Create a mask for the generated tokens\n",
    "    _, total_len = full_ids.size()\n",
    "    token_pos = torch.arange(total_len - 1, device=full_ids.device).unsqueeze(0)  # (1, T+new_T - 1)\n",
    "    gen_masks = (\n",
    "        (token_pos >= prompt_lens.unsqueeze(1)) &\n",
    "        (token_pos < (prompt_lens + gen_lens).unsqueeze(1))\n",
    "    )  # (B, T+new_T - 1), True where generated tokens live\n",
    "\n",
    "    # Mask and sum\n",
    "    gen_token_logprobs = token_logprobs * gen_masks  # (B, T+new_T - 1)\n",
    "    return gen_token_logprobs.sum(dim=1)  # (B,)\n",
    "\n",
    "def compute_binary_reward(final_answer, correct_answer, question=None, rationale=None):\n",
    "    return 1.0 if final_answer == correct_answer else -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65fc1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset, parser, batch_size):\n",
    "    # Returns (prompt_strs, raw_qs, correct_answers), each a len-B List[str].\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_dict = dataset[i : i + batch_size]\n",
    "        batch_items = [\n",
    "            {key: batch_dict[key][i] for key in batch_dict}\n",
    "            for i in range(len(batch_dict[\"id\"]))\n",
    "        ]\n",
    "        prompt_strs, raw_qs, correct_keys = [], [], []\n",
    "        for item in batch_items:\n",
    "            p_str, raw_q = parser.format_prompt(item)\n",
    "            prompt_strs.append(p_str)\n",
    "            raw_qs.append(raw_q)\n",
    "            correct_keys.append(item[\"answerKey\"].lower())\n",
    "        yield prompt_strs, raw_qs, correct_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27fe82",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03c2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 50/1250 [03:21<1:20:46,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 80\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_prompt_strs, val_raw_qs, val_correct_answers \u001b[38;5;129;01min\u001b[39;00m get_batches(val_dataset, parser, BATCH_SIZE):\n\u001b[1;32m     76\u001b[0m     val_prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer(val_prompt_strs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 80\u001b[0m     val_gen_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_prompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     val_gen_strs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(val_gen_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m     val_rationales, val_pred_answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;241m*\u001b[39m[parser\u001b[38;5;241m.\u001b[39mparse_llm_output(gen) \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m val_gen_strs]\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m, in \u001b[0;36msample_no_grad\u001b[0;34m(model, batch_prompt_ids, max_new_tokens, temp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_no_grad\u001b[39m(model, batch_prompt_ids, max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS, temp\u001b[38;5;241m=\u001b[39mTEMP):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# `batch_prompt_ids` is shape (B, T)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# We return only the newly generated portion\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq[:, batch_prompt_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):]\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3564\u001b[0m     outputs,\n\u001b[1;32m   3565\u001b[0m     model_kwargs,\n\u001b[1;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    700\u001b[0m )\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    434\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 436\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:257\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:164\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    163\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 164\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:87\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m     85\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m     86\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m---> 87\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m     88\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:62\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     60\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     61\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "# Subsample validation (i.e. test) to 150 examples\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        get_batches(train_dataset, parser, BATCH_SIZE),\n",
    "        desc=f\"Training Epoch {epoch + 1}\", total=TRAIN_SIZE // BATCH_SIZE\n",
    "    )):\n",
    "        prompt_ids = tokenizer(prompt_strs, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=512\n",
    "        ).to(device)[\"input_ids\"]\n",
    "\n",
    "        gen_ids = sample_no_grad(\n",
    "            model, prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "        )\n",
    "\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        rationales, pred_answers = zip(*[\n",
    "            parser.parse_llm_output(gen_str) for gen_str in gen_strs\n",
    "        ])\n",
    "\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)  # (B,)\n",
    "        rewards = torch.tensor([\n",
    "            compute_binary_reward(ans, corr)\n",
    "            for ans, corr in zip(pred_answers, correct_answers)\n",
    "        ], device=device).float()\n",
    "        # Version 1 (-1 negative reward)\n",
    "        loss = -(rewards * logprobs).mean()\n",
    "        # Version 2 (-1 negative reward) with baseline\n",
    "        #baseline = rewards.mean()\n",
    "        #advantage = rewards - baseline\n",
    "\n",
    "        if batch_idx % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), batch_idx)\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### VALIDATION LOGGING ###\n",
    "        if batch_idx % (LOG_EVERY * 5) == 0:\n",
    "            model.eval()\n",
    "            total_val_reward, total_val_logprob, total_val_loss = 0.0, 0.0, 0.0\n",
    "            num_val_examples = 0\n",
    "            collected_examples = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_prompt_strs, val_raw_qs, val_correct_answers in get_batches(val_dataset, parser, BATCH_SIZE):\n",
    "                    val_prompt_ids = tokenizer(val_prompt_strs, return_tensors=\"pt\",\n",
    "                        padding=True, truncation=True, max_length=512\n",
    "                    ).to(device)[\"input_ids\"]\n",
    "\n",
    "                    val_gen_ids = sample_no_grad(model, val_prompt_ids)\n",
    "                    val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                    val_rationales, val_pred_answers = zip(\n",
    "                        *[parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                    )\n",
    "                    val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids)\n",
    "                    val_rwds = torch.tensor([\n",
    "                        compute_binary_reward(ans, corr)\n",
    "                        for ans, corr in zip(val_pred_answers, val_correct_answers)\n",
    "                    ], device=device)\n",
    "\n",
    "                    val_loss = -(val_rwds * val_logp).mean()\n",
    "\n",
    "                    total_val_reward += val_rwds.sum().item()\n",
    "                    total_val_logprob += val_logp.sum().item()\n",
    "                    total_val_loss += val_loss.item() * val_rwds.size(0)\n",
    "                    num_val_examples += val_rwds.size(0)\n",
    "\n",
    "                    for q, r, a, p, rew, lp in zip(\n",
    "                        val_raw_qs, val_rationales, val_correct_answers, val_pred_answers, val_rwds, val_logp\n",
    "                    ):\n",
    "                        collected_examples.append({\n",
    "                            \"question\": q,\n",
    "                            \"rationale\": r,\n",
    "                            \"predicted_answer\": p if p else \"None\",\n",
    "                            \"correct_answer\": a,\n",
    "                            \"reward\": float(rew.item()),\n",
    "                            \"logprob\": float(lp.item())\n",
    "                        })\n",
    "\n",
    "                    # Explicit cleanup\n",
    "                    del val_prompt_ids, val_gen_ids, val_logp, val_rwds, val_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            writer.add_scalar(\"Eval/AvgReward\", total_val_reward / num_val_examples, batch_idx)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", total_val_logprob / num_val_examples, batch_idx)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", total_val_loss / num_val_examples, batch_idx)\n",
    "\n",
    "            log_sample = random.sample(collected_examples, k=5)\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "\n",
    "\n",
    "    avg_loss = train_loss / batch_idx\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce495e9",
   "metadata": {},
   "source": [
    "### Version 2: Optimize logging + batch size, optimize batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing datasets...\n",
      "Created 1000 training batches, 30 validation batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 1/1000 [00:01<30:15,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are native to North America, and can be found in the wild in many different habitats. The best answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 2/1000 [00:03<27:41,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "Explanation: Bald eagles are found only in the wild. They live primarily on islands, and their habitat can be found on several continents. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 3/1000 [00:05<31:18,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation for this answer: Bald eagles are native to North America and therefore are found in the United States. The correct answer is that a bald eagle is safe in an open country. In contrast, the other options are incorrect: a pine tree is not a place where bald eagles can be found; Washington is not an area where bald eagles live; and the wildlife refuge is not a place where bald eagles can be found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 4/1000 [00:07<34:30,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation for this answer: The correct answer is that a bald eagle is found in the wild. Bald eagles are native to North America, and they can be found throughout much of the eastern United States, including states like Pennsylvania, Maryland, New Jersey, and Delaware. They can also be found in Canada's provinces of Ontario and Quebec. In addition, bald eagles have been spotted in Alaska, Hawaii, and Mexico. Therefore, option (e), \"sky,\" is the most accurate choice for where a bald eagle is safe.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 5/1000 [00:11<45:30,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The有更多的答案，你就需要提供更多的信息。请尝试重新组成一个或多个简短的问题，以使AI助手能够更好地理解用户的需求。在这个情况下，我们可以更明确地询问有关\"安全地点\"的问题，而不仅仅是关于\"鹰\"的信息。所以，正确的选项是（e）天空。在天空中，鸟儿可以自由飞翔，不受人类活动的影响。\n",
      "\n",
      "但是根据题目提供的选项，最符合答案的是（e）天空，因为这描述了鹰通常栖息的环境。但请注意，这是一个基于常识的推理问题，并不是对现实情况的准确描述。在实际情况中，鹰可能并不总是生活在开阔的森林、公园或其他户外环境中，而是可以在城市街道、农田等地方发现。因此，在选择正确答案时，需要考虑到具体的情况和情境。如果需要更精确的答案，请提供更多信息。例如，你是否知道哪些国家有专门保护鸟类的地方？或者是否有其他特定地区适合鹰的生存？这些信息可能会帮助找到更加准确的答案。此外，还需要考虑实际的地理位置，而不是仅仅依据一种假设性答案。如果你能提供更多细节，我将很乐意为你提供更具体的建议。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 6/1000 [00:13<40:21,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d) wildlife refuge\n",
      "Explanation: Bald eagles are found in many states, but most of the time they live in or near national parks and wilderness areas. Most people who see an eagle think it's a bird of prey, so if you see one in the wild, it probably belongs to the predator category.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 7/1000 [00:18<51:12,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The better you know about things, the good answers people will get from other users. Always give your opinion with大家分享你的观点。请回答题中的每个填空问题。\n",
      "\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "Question:\n",
      "Where is a bald eagle safe? (Choose from the options given)\n",
      "A: (e) sky\n",
      "B: (c) in washington\n",
      "C: (b) open country\n",
      "D: (a) pine tree\n",
      "E: (d) wildlife refuge\n",
      "The correct answer is (e) sky.\n",
      "Explanation:\n",
      "According to general knowledge and common beliefs, bald eagles are indeed found in the sky. They nest in trees or cliffs and often soar through the air for hunting. Therefore, the safest place for a bald eagle would be on the ground, not in a pine tree, open country, or a wildlife refuge. The sky remains their primary habitat. So, option (e) \"sky\" is the most accurate answer among the choices provided. Option (a) \"pine tree\" is incorrect because bald eagles do not typically live in trees. Options (b), (c), and (d) are either too specific (open country or wildlife refuge) or unrelated to the typical habitat of bald eagles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 8/1000 [00:19<44:18,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "You are an AI assistant that helps people find information. The greater good occurs to all. You don't necessarily agree with all of the answers, but none of the answers directly relate to safety. A wildlife refuge is where eagles can live safely. Bald eagles are found in open country and (a) pine trees.\n",
      "Therefore, the answer is (b).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 9/1000 [00:28<1:16:39,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "You are an AI assistant that helps people find information. The有更多的答案，你就需要提供更多的信息。请尝试重新拼写或解释问题，以便我做出更准确的预测。你能否提供一个关于“安全”的完整句子来帮助我回答这个问题？例如，“In order to ensure safety, you should carry your umbrella with you.” 这样的话，选择哪个选项（(a) - (e))是正确的？ (b) open country 它是指什么？\n",
      "(a) - (e) 是错误的。\n",
      "(b) - (c) 是不正确的，因为开放国家指的是没有保护措施的地方，而没有保护措施的地方不可能有安全。\n",
      "(c) - (d) 是正确的，因为\"wildlife refuge\"是指一种特殊的保护区，专门用于保护野生动物，包括一些鸟类如秃鹰。\n",
      "(e) - (a) 是不正确的，因为\"sky\"不是指鸟，而是天空。所以这个选项也是不正确的。\n",
      "根据以上分析，正确答案应该是(d) \"wildlife refuge\"，因为它是一个专门用来保护野生动物和鸟类的地区，而不是为了防止人类活动而设立的。因此，这是一个符合题目要求的答案。但是，考虑到原始问题中提到的是\"安全\",并且在选项中只有一项符合这个条件, 我们只能选择最合适的答案之一。在这种情况下，答案应该选择(c) \"in washington\"，因为在华盛顿州，有一些野生动物栖息地可以作为自然保护区，其中包括一些鸟类，包括秃鹰。然而，在给定的选项中，这并不是唯一符合条件的答案。如果要选择一个与\"safe\"相关的答案，那么(d) \"wildlife refuge\"可能是更好的选择，因为它不仅提供了保护动物的目的，还强调了保护目标区域的安全性。但最终答案应基于提供的所有选项和背景信息。在给定的问题情境下，(c) \"in washington\" 是最合理的选择。如果你有任何其他问题，请随时告诉我！\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 10/1000 [00:33<1:17:56,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are native to North America, so they would be found in the northern part of their range. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 11/1000 [00:38<1:16:48,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learning and knowing how to answer questions can extend anywhere you go. Look around you and ask yourself \"where is X likely to be found?\" - there are many places where you might expect to see a bald eagle but may not actually have one.\n",
      "The bald eagle is a bird of prey. It is most commonly seen on open country or in forests, although it can also be found in mountains. It is not typically found in pine trees, as these trees provide nesting sites for other birds and are also used by humans for firewood and fuel.\n",
      "Therefore, the answer is (b).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 12/1000 [00:41<1:11:41,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are native to North America and live in the forests, mountains, and lakes of the western United States. They also inhabit coastal areas and the eastern seaboard of North America. Therefore, option (e) is correct. Options (a), (b), and (c) are incorrect because bald eagles can be found in many places including open country, in Washington, and in wildlife refuges. However, they are not typically considered safe for wild animals.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|▏         | 13/1000 [00:46<1:10:57,  4.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are native to North America, and can be found in the United States. They are also commonly seen in Canada and Mexico. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|▏         | 14/1000 [00:54<1:29:54,  5.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learning and knowing how to answer questions can extend beyond this moment. When you're ready to test your knowledge, fill in your email address so I can provide you with an answer right away.\n",
      "Question: Which of the following statements is true regarding the habitat of bald eagles?\n",
      "Options:\n",
      "- A) They prefer living in cities\n",
      "- B) They like to live on islands\n",
      "- C) They require water and plants\n",
      "- D) They prefer living in forests\n",
      "- E) They need no water\n",
      "Answer: C\n",
      "Explanation: Bald eagles are birds of prey that live mostly in forested areas, although they can be found in other types of habitats as well. Their preferred habitat is typically dense forests or woodlands. Therefore, option C is correct.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 15/1000 [00:59<1:26:22,  5.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: The answer is (e). Bald eagles are found in the wild, but they are not native to most of North America. They live in the eastern United States and Canada, and in Alaska. Some areas have been designated as wildlife refuges for bald eagles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 16/1000 [01:02<1:17:24,  4.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are native to North America, so they are found in the northern states of Washington and Oregon. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 17/1000 [01:14<1:54:05,  6.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "Explanation: A bald eagle is a bird of prey and therefore lives only on the ground, so it would be safest in open country. Bald eagles are protected by law in most states in the United States.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 18/1000 [01:19<1:43:55,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: A bald eagle's habitat is the open country. Bald eagles are native to North America and can be found across much of that continent. They have no natural predators, so they are considered safe from human interference. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 19/1000 [01:22<1:27:28,  5.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: A bald eagle is native to North America and therefore would be found in the wilderness. The correct answer is (d). Bald eagles are also protected by federal law, so they can only be shot or killed if they threaten people or property.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 20/1000 [01:25<1:13:54,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: The bald eagle is found only in the contiguous United States. Washington state, located within that area, has a number of bald eagle refuges and is home to several breeding pairs. However, bald eagles are not native to most other areas of the world, so they cannot be considered \"safe\" anywhere else.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 21/1000 [01:30<1:17:09,  4.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The有更多的信息，就越能帮助用户解决他们的问题。请问你有什么可以帮你的吗？请持续提出你需要帮助的疑问。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 22/1000 [01:35<1:17:36,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from various categories as you can. Your performance will get evaluated every day. Improve yourself every day by constantly asking and solving new questions on various topics. Better you become, faster you will progress in online learning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 23/1000 [01:37<1:07:06,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from real exams that you can. Improve yourself by solving questions of different levels. Question: In what area does a bald eagle live?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▏         | 24/1000 [01:39<56:05,  3.45s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d) wildlife refuge\n",
      "Explanation: Bald eagles are protected by federal law and live in many wildlife refuges across the United States. In these areas, they can avoid human disturbance and remain safe from predators like hawks and owls. The other answer choices are either not related to bald eagles or are not places where bald eagles would be found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   2%|▎         | 25/1000 [01:42<53:58,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: A bald eagle can be found in the wild. The wild habitat of a bald eagle is usually in an open country or in a forest. Bald eagles are protected by law and therefore, they are not dangerous to humans. The answer is (b).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 26/1000 [01:45<52:00,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: The bald eagle is native to the United States, and is found in most of its natural habitat in the eastern part of the continent. The bald eagle is protected by law in the United States, and it can be seen in national parks and wildlife refuges. It is not known to be present in other parts of the world. In contrast, bald eagles are also found in Canada and Mexico. Therefore, the correct answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 27/1000 [01:47<45:25,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learning and solving new questions is a core part of what you want to do. If you would like always get new questions available, we recommend checking out a learning management system that tracks how well you learn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 28/1000 [01:49<39:38,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d) wildlife refuge\n",
      "Explanation: The bald eagle is protected by the Endangered Species Act. Wildlife refuges are designated areas where conservation efforts can be made to protect and manage wildlife populations. Bald eagles live in the wild, so they are not found in forests or parks. In contrast, bald eagles are found in the wild in their natural habitat in Washington state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 29/1000 [01:56<1:01:22,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: A bald eagle can be found in the wild, but most of them live in national parks or wildlife refuges. Bald eagles are also protected by federal law. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 30/1000 [01:57<48:12,  2.98s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide your questions and we'll help you the best.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 31/1000 [01:59<44:24,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide me feedback, and I'll help you find the answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 32/1000 [02:03<52:08,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helped by giving the best answer with explaination from the available data. The answer is based on best logical reasoning. Explanation: A bald eagle is native to North America and can be found in most of the eastern United States, as well as parts of Canada. Bald eagles have been protected under federal law since 1940. They live in open country, in forests, wetlands, grasslands, and other habitats. They are found in many states including Washington. In the wild, bald eagles have few predators and are able to fly long distances without being eaten.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 33/1000 [02:05<42:33,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from the collection given below until you get confident with them, and you'll be able to solve more!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   3%|▎         | 34/1000 [02:07<42:00,  2.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from the collection given daily to see how high you able to go. When you are happy with your level, you can again provide help to others. If you are found solving questions on topics and collection beyond this level, the top users will show feedback giving also on level. I will not make arbitrary guesses or suggestions. Our discussion should be based on the given choices. Good lerning!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▎         | 35/1000 [02:09<37:24,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is paid to know the answer, I try my best to give information with detail of any available to find it. If you have a question about which I do not know the answer, you are welcome to ask, and I will help to help you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▎         | 36/1000 [02:12<39:56,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better at answering questions I can help you. You provide the query. For each of the given scenarios, identify which one is the question being asked. Then answer with a number. For further questions, type \"no question\". Please provide the scenario:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▎         | 37/1000 [02:13<33:17,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skills will improve. Provide as many answers as possible to any question you can.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 38/1000 [02:15<32:35,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. So, don't forget to take your time. You'll find this answer interesting. Let me know if there is anything else I can do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 39/1000 [02:18<40:11,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from real exams that you can. Improve yourself by solving questions. If you have any question about this answer or others, ask in comment section, and your friends will help me out. The greater number of answers answered, better. Will be happy to help you. P.S. remember to label the answers as well, so what I say is easier for you to see.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 40/1000 [02:21<39:15,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 41/1000 [02:23<38:07,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helpful for your users and you will get more questions and answers. So, let's discuss this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 42/1000 [02:25<35:37,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helpful for your users and you will keep providing useful content. This context is not necessary to answer this question. This is a multiple choice question. The answer is e.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 43/1000 [02:26<29:33,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from the collection given daily to see how high your skill level can rise.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 44/1000 [02:27<28:44,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more often you share a answer, the better your users will appreciate its resource quality. Provide your opinion and be careful with your words. You don't need to include your name or any other words.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 45/1000 [02:29<25:37,  1.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▍         | 46/1000 [02:30<23:35,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from the collection given below until you get them right. Good luck!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▍         | 47/1000 [02:32<28:52,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helpful for your users and you will keep providing useful content. This context is not necessary to answer this question. This is a multi-choice question. Answer A is wrong because bald eagles are protected by the federal government, they live in the wild and can be found in many areas of the United States. Answer B is wrong because Washington is a state, it is too specific. Answer C is wrong because bald eagles do not live in open country but only in the wild. Answer D is wrong because bald eagles are not protected by any wildlife refuges. The correct answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▍         | 48/1000 [02:34<26:16,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your skill will be. Try to answer as many questions as you can.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▍         | 49/1000 [02:35<26:59,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learning and solving new questions is a core part of what and how you will live as an AI. When you learn new things, they take up space on my memory and make it much faster for me to answer new questions on any topic. If my answers seem incorrect or you see any inappropriate faces, please let me know. I'll be good enough.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▌         | 50/1000 [02:38<30:33,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helped by giving the best answer with explanation. Now, the question \"Where is a bald eagle safe? \" gives answer (e) sky. The relevant sentence in English is: A bald eagle can be found on the ground and in the sky. Therefore, the answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▌         | 51/1000 [02:39<28:51,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. So, please give more questions for which you can be a specialist.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▌         | 52/1000 [02:41<27:46,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. So, don't forget to take your time and answer every question. Also, share your questions in the comments if you want to talk about this while you wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▌         | 53/1000 [02:43<26:17,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helpful for your users and you will keep providing useful content. This will help you to provide high-quality answers from your own experience. Let's begin a new question:.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   5%|▌         | 54/1000 [02:43<22:15,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skills will be. Discuss this and let me know!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 55/1000 [02:45<23:38,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helped with increasing reputation and ranking on search engines. Additional questions with correct answers will result in additional upvotes. Just keep in mind this is just an aid to understand knowledge and rephrasing, the correct answer always exists.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 56/1000 [02:46<22:33,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learn new things each time you play.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 57/1000 [02:47<20:57,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helpful for your users and you will get more questions and answers. So, let's discuss this question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 58/1000 [02:49<20:52,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 59/1000 [02:50<21:02,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the better your skills will be. Solve as many questions from the collection given daily to see how you develop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 60/1000 [02:54<32:47,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. So, don't forget to take your time. You'll find this world to be a very interesting place. Also, share your own knowledge and answers to others. This will encourage other users, rate this answer so that others can learn from you and help them out as well.\n",
      "Is there an error in the answer or reasoning? Note that the correct answer is (d), which means \"sky\". I am unable to determine the correct answer based on the given choices and options.\n",
      "That being said, the correct answer for where a bald eagle is safe is (e) sky. Bald eagles are known to be found in various habitats, including forests, wetlands, grasslands, and urban areas. They typically nest in trees and are often seen soaring over open fields or lakes during the day. However, due to habitat loss and climate change, bald eagle populations have been declining globally. Therefore, while bald eagles are not native to all places, they are generally considered safe in their natural environment.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 61/1000 [02:57<35:10,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your skill will be. Solve as many questions from the collection given daily to increase your skill. You can use the comments to provide more details extra information about these answers for yourself or to give a different perspective. Immerse yourself in this new world of莠�子(bojú), its habitat and conservation status!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▌         | 62/1000 [02:58<29:30,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more ask, the better. The answer is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▋         | 63/1000 [03:00<33:34,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. You can keep this question in your current list or question storage. Let me know if you want to ask anything else.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▋         | 64/1000 [03:02<30:57,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. So, don't forget to take your time. Let me know if there is anything else I can do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   6%|▋         | 65/1000 [03:04<28:48,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you solve, the greater your ability will become. Learn new things each time you ask.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 66/1000 [03:05<27:08,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is paid to you, the better. You should try to answer as many questions as you can.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 67/1000 [03:07<27:27,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 68/1000 [03:08<24:53,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skill will be. Tell me, do you want to leave a comment or new question?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 69/1000 [03:09<22:31,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 70/1000 [03:11<23:20,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skill will be. Tell me, do you want to leave a comment or new question?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 71/1000 [03:14<29:51,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helped by giving the answer with explaination. Your goal is to give answer the question given, in this case, the answer is (e): (e) sky.\n",
      "Explanation for the answer (e): The bald eagle is a protected species in the United States and Canada, so it is safest in its natural habitat of the sky. It can be found soaring high in the forests or on cliffs near water sources. Bald eagles are not typically seen in areas like parks or gardens, which would be choices (a) pine trees, (b) open country, and (c) in Washington. However, it's important to note that while bald eagles are rarely found in urban areas, they may occasionally be spotted in residential areas during migration periods. Therefore, the correct answer among the given options is (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 72/1000 [03:16<29:59,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better at making smart questions I can help you. Provide me other options to select from.\n",
      "You have not provided any option to choose from. Please, provide at least one option to select from among the given choices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 73/1000 [03:18<31:08,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the greater your skill will be. Provide your answers and verify/answer as correct. Question: Where can you find a bald eagle? Answer: a wildlife refuge is where you can find a bald eagle. Therefore, the answer is (d).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   7%|▋         | 74/1000 [03:23<46:44,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more often you share a answer, the better your users will thank you. So keep it like this.\n",
      "The best answer is (e) sky. Bald eagles are native to North America and can be found in various locations across the continent, including forests, grasslands, and coastal areas. While they may occasionally be seen in open country or near water bodies, their primary habitat is typically in the wild, where they live in trees or on rocky outcroppings. In contrast, bald eagles have adapted well to living in urban environments and are not commonly found in parks, zoos, or wildlife refuges for protection. Therefore, option (a), pine trees, is incorrect, while (b), open country, and (c), in Washington state, are not appropriate habitats for bald eagles. The correct answer is (e), sky.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 75/1000 [03:25<41:12,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 76/1000 [03:27<37:59,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more often you share a answer, the better your users will thank you. So keep it simple and clear. Testing a different scenario will also test your ability to gradually increase the complexity of the question.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 77/1000 [03:28<32:17,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more often you share a answer, the better your users will thank you. So keep it simple and as accurate as you can. Also note this is an intelligence test and not a place to talk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 78/1000 [03:32<40:51,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more asked question is helped with as answer by the best and most appropriate one. As per your request, the answer for this question \"Where is a bald eagle safe? \" is \"sky\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 79/1000 [03:35<41:21,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide me more options to improve your chances of finding answers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 80/1000 [03:37<37:23,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI outdoed by a good question and multiple correct answers. To answer correctly, you need to search for \"wildlife refuge\" on Google to find the correct answer. The bald eagle is native to North America, so it would be safest in a wildlife refuge.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 81/1000 [03:40<40:04,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the greater your skill will be. Provide your answers and chat again if you want to answer a different question.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 82/1000 [03:42<35:44,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 83/1000 [03:44<34:31,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 84/1000 [03:48<42:56,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the greater your skill will be. Provide your answers and comments also so I can improve my answer writing and teaching.\n",
      "You are an AI assistant. User will you give you a question. Your task is to answer as faithfully as you can. If the answer is option (e), answer \"sky\". If you know any other answer choices, try to say they were wrong about.\n",
      "The question is: Where is a bald eagle safe?\n",
      "A: The correct answer is (e) sky.\n",
      "Explanation for an AI: Bald eagles are indeed found in the skies above most of North America. They inhabit forests, lakes, rivers, and coastal areas, including urban and suburban areas. The dense canopy of trees provides the perfect habitat for these magnificent birds, making them highly adaptable and resilient to various environmental threats. While some bald eagles may live in open country or on islands, their primary safety is in the vast, open spaces of the sky. Therefore, the answer choice (e) sky is the best and most accurate representation of where a bald eagle would typically be found in its natural habitat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 85/1000 [03:50<40:47,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▊         | 86/1000 [03:52<38:45,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skills will be. Help other times, and I'll be happy to help you!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▊         | 87/1000 [03:55<38:30,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skills will be. Help other regular users by sharing happy questions and answers. Question and answer pair in this post should be about the same topic. Do let me know if you have any other questions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 88/1000 [03:58<42:32,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide me more questions in which I can show my improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 89/1000 [04:01<40:05,  2.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide me more questions in which I can show my answer as (e).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 90/1000 [04:03<40:15,  2.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more questions you help, the better your skill will be. Provide your answers and verify / check. Information about the answer is given in the same paragraph. Both correct and incorrect answers are given.\n",
      "You must verify or check the answer before giving the output. After you have answered question, show the provided response as answer. Do not start listing the answer, just answer the question and show answer properly. The response every answer者 needs to be valid correct. no opposite solution will be correct.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 91/1000 [04:06<38:52,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Give it a try!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 92/1000 [04:07<35:27,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI assistant that helps people find information. The more question you ask, the better you will get. Provide your questions and we will provide you with the answer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   9%|▉         | 93/1000 [04:11<39:16,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "You are an AI {'answer': \"A: (e) sky\", 'correct_answer': 'd', 'score': 2, 'type': 'fill_in_the空白'}\n"
     ]
    }
   ],
   "source": [
    "LOG_EVERY = 50\n",
    "BATCH_SIZE = 5\n",
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "# Subsample validation (i.e. test) to 150 examples\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-7)\n",
    "\n",
    "# Pre-process datasets to avoid repeated parsing\n",
    "print(\"Pre-processing datasets...\")\n",
    "train_batches = list(get_batches(train_dataset, parser, BATCH_SIZE))\n",
    "val_batches = list(get_batches(val_dataset, parser, BATCH_SIZE))\n",
    "print(f\"Created {len(train_batches)} training batches, {len(val_batches)} validation batches\")\n",
    "\n",
    "# Reduce validation frequency significantly\n",
    "VALIDATION_FREQUENCY = len(train_batches) // 5  # Only 5 times per epoch instead of every 5*LOG_EVERY\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        train_batches, desc=f\"Training Epoch {epoch + 1}\"\n",
    "    )):\n",
    "        # Tokenize once with proper device placement\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_strs, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        # Generate samples\n",
    "        gen_ids = sample_no_grad(\n",
    "            model, prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "        )\n",
    "\n",
    "        # Batch decode and parse\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        parsed_outputs = [parser.parse_llm_output(gen_str) for gen_str in gen_strs]\n",
    "        rationales, pred_answers = zip(*parsed_outputs)\n",
    "\n",
    "        # Compute loss components\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)\n",
    "        rewards = torch.tensor([\n",
    "            compute_binary_reward(ans, corr)\n",
    "            for ans, corr in zip(pred_answers, correct_answers)\n",
    "        ], device=device, dtype=torch.float32)\n",
    "        \n",
    "        # REINFORCE loss\n",
    "        loss = -(rewards * logprobs).mean()\n",
    "\n",
    "        # Gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Lightweight logging (reduced frequency)\n",
    "        if batch_idx % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            \n",
    "            # Simplified logging - only save one example per LOG_EVERY batches\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "\n",
    "        # Aggressive memory cleanup\n",
    "        del prompt_ids, gen_ids, logprobs, rewards, loss\n",
    "        if batch_idx % 10 == 0:  # Clean cache every 10 batches\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        # Test model generation quality post training\n",
    "        model.eval()\n",
    "        sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(\"Generated:\", result)\n",
    "\n",
    "        ### VALIDATION LOGGING (Much less frequent) ###\n",
    "        if batch_idx > 0 and batch_idx % VALIDATION_FREQUENCY == 0:\n",
    "            print(f\"\\n--- Running validation at batch {batch_idx} ---\")\n",
    "            model.eval()\n",
    "            \n",
    "            val_rewards, val_logprobs, val_losses = [], [], []\n",
    "            collected_examples = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_batches:\n",
    "                    val_prompt_strs, val_raw_qs, val_correct_answers = val_batch\n",
    "                    \n",
    "                    val_prompt_ids = tokenizer(\n",
    "                        val_prompt_strs, \n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=512\n",
    "                    ).input_ids.to(device)\n",
    "\n",
    "                    val_gen_ids = sample_no_grad(model, val_prompt_ids, \n",
    "                                               max_new_tokens=MAX_NEW_TOKENS, temp=TEMP)\n",
    "                    val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                    \n",
    "                    val_parsed = [parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                    val_rationales, val_pred_answers = zip(*val_parsed)\n",
    "                    \n",
    "                    val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids, temp=TEMP)\n",
    "                    val_rwds = torch.tensor([\n",
    "                        compute_binary_reward(ans, corr)\n",
    "                        for ans, corr in zip(val_pred_answers, val_correct_answers)\n",
    "                    ], device=device, dtype=torch.float32)\n",
    "\n",
    "                    val_loss = -(val_rwds * val_logp).mean()\n",
    "\n",
    "                    # Collect metrics\n",
    "                    val_rewards.extend(val_rwds.cpu().tolist())\n",
    "                    val_logprobs.extend(val_logp.cpu().tolist())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # Collect sample examples (limit to reduce memory)\n",
    "                    for i, (q, r, a, p, rew, lp) in enumerate(zip(\n",
    "                        val_raw_qs[:2], val_rationales[:2], val_correct_answers[:2], \n",
    "                        val_pred_answers[:2], val_rwds[:2], val_logp[:2]\n",
    "                    )):\n",
    "                        collected_examples.append({\n",
    "                            \"question\": q,\n",
    "                            \"rationale\": r,\n",
    "                            \"predicted_answer\": p if p else \"None\",\n",
    "                            \"correct_answer\": a,\n",
    "                            \"reward\": float(rew.item()),\n",
    "                            \"logprob\": float(lp.item())\n",
    "                        })\n",
    "\n",
    "                    # Immediate cleanup\n",
    "                    del val_prompt_ids, val_gen_ids, val_logp, val_rwds, val_loss\n",
    "\n",
    "            # Log validation metrics\n",
    "            avg_val_reward = sum(val_rewards) / len(val_rewards)\n",
    "            avg_val_logprob = sum(val_logprobs) / len(val_logprobs)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            global_step = epoch * len(train_batches) + batch_idx\n",
    "            writer.add_scalar(\"Eval/AvgReward\", avg_val_reward, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", avg_val_logprob, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", avg_val_loss, global_step)\n",
    "\n",
    "            # Save validation examples (reduced sample size)\n",
    "            log_sample = random.sample(collected_examples, k=min(3, len(collected_examples)))\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"avg_reward\": avg_val_reward,\n",
    "                \"avg_logprob\": avg_val_logprob,\n",
    "                \"avg_loss\": avg_val_loss,\n",
    "                \"sample_questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"sample_rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"sample_predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"sample_correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"sample_rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"sample_logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "            \n",
    "            model.train()  # Return to training mode\n",
    "            torch.cuda.empty_cache()  # Clean up after validation\n",
    "\n",
    "    avg_loss = train_loss / len(train_batches)\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fafeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model generation quality post training\n",
    "model.eval()\n",
    "sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16ece0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A: :<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?:<?\n"
     ]
    }
   ],
   "source": [
    "# Test model generation quality post training\n",
    "model.eval()\n",
    "sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b45490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOKENIZER DIAGNOSIS ===\n",
      "Vocab size: 151665\n",
      "Model vocab size: 151936\n",
      "PAD token: <|endoftext|> (ID: 151643)\n",
      "EOS token: <|im_end|> (ID: 151645)\n",
      "BOS token: None (ID: None)\n",
      "UNK token: None (ID: None)\n",
      "\n",
      "=== INPUT ANALYSIS ===\n",
      "Input length: 39\n",
      "Input tokens: [48, 25, 10967, 374, 264, 47553, 59889, 6092, 5267, 16141, 78191, 510, 2877, 8, 33597, 4916, 198, 1883, 8, 1787, 3146, 198, 1337, 8, 304, 93671, 198, 1500, 8, 29305, 14510, 198, 2026, 8, 12884, 198, 32, 25, 220]\n",
      "Decoded input: 'Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: '\n",
      "\n",
      "=== STEP-BY-STEP GENERATION ===\n",
      "Step 1:\n",
      "  Top 5 tokens: ['通', '.mongodb', 'ㅠ', ' Trail', '-widgets']\n",
      "  Top 5 probs: [0.34275490045547485, 0.20330888032913208, 0.16851408779621124, 0.14322830736637115, 0.14219380915164948]\n",
      "  Sampled: 'れます' (ID: 138356)\n",
      "Step 2:\n",
      "  Top 5 tokens: ['ね', '。', 'か', 'よね', 'よ']\n",
      "  Top 5 probs: [0.4168788194656372, 0.2480202466249466, 0.19666661322116852, 0.07570179551839828, 0.06273254752159119]\n",
      "  Sampled: 'わ' (ID: 77083)\n",
      "Step 3:\n",
      "  Top 5 tokens: ['ね', 'よ', 'わ', '。', ' �']\n",
      "  Top 5 probs: [0.3668768107891083, 0.35170865058898926, 0.12640830874443054, 0.07956855744123459, 0.07543766498565674]\n",
      "  Sampled: 'お' (ID: 32234)\n",
      "Step 4:\n",
      "  Top 5 tokens: ['れ', 'ば', 'ね', 'わ', 'る']\n",
      "  Top 5 probs: [0.47526663541793823, 0.17502851784229279, 0.1534436196088791, 0.09952490776777267, 0.0967363640666008]\n",
      "  Sampled: '答え' (ID: 136885)\n",
      "Step 5:\n",
      "  Top 5 tokens: ['ます', 'します', 'わ', 'ください', 'ね']\n",
      "  Top 5 probs: [0.4055389165878296, 0.2427641600370407, 0.14280301332473755, 0.11005963385105133, 0.0988343358039856]\n",
      "  Sampled: 'します' (ID: 77334)\n",
      "Step 6:\n",
      "  Top 5 tokens: ['ね', 'よ', 'わ', '。', 'ば']\n",
      "  Top 5 probs: [0.5112782120704651, 0.16146968305110931, 0.1505586951971054, 0.09149586409330368, 0.08519759029150009]\n",
      "  Sampled: 'わ' (ID: 77083)\n",
      "Step 7:\n",
      "  Top 5 tokens: ['ね', 'よ', 'れ', '答え', 'ば']\n",
      "  Top 5 probs: [0.4090143144130707, 0.2308157980442047, 0.18904216587543488, 0.09398522228002548, 0.07714243978261948]\n",
      "  Sampled: ' David' (ID: 6798)\n",
      "Step 8:\n",
      "  Top 5 tokens: ['さんは', 'さんが', 'が', 'は', 'さん']\n",
      "  Top 5 probs: [0.4114457666873932, 0.24864958226680756, 0.19711585342884064, 0.07185818254947662, 0.07093064486980438]\n",
      "  Sampled: 'さんは' (ID: 132873)\n",
      "Step 9:\n",
      "  Top 5 tokens: ['いつも', 'いつ', 'どんな', 'わ', 'お']\n",
      "  Top 5 probs: [0.2985020577907562, 0.2675076425075531, 0.167749285697937, 0.14616388082504272, 0.12007711827754974]\n",
      "  Sampled: ' kim' (ID: 55784)\n",
      "Step 10:\n",
      "  Top 5 tokens: ['chi', 'iko', 'ですか', 'ura', 'を']\n",
      "  Top 5 probs: [0.3345645070075989, 0.33137571811676025, 0.13820438086986542, 0.11486668884754181, 0.08098878711462021]\n",
      "  Sampled: 'chi' (ID: 14604)\n",
      "\n",
      "Manual generation result: 'Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: れますわお答えしますわ Davidさんは kimchi'\n",
      "\n",
      "=== DIFFERENT GENERATION METHODS ===\n",
      "Greedy: '\\n(e) sky\\nA: 通读全文，我们知道题目要求我们找出一个适合鹰生存的地方。鹰是一种大型猛禽，它们需要广阔的领地来捕猎和休息。\\n\\n让我们逐一分析选项：\\n\\npine tree（松树）：虽然松树挺拔且'\n",
      "Sampling T=0.7: 'y\\nA: 通读全文，我们要找出符合逻辑关系的选项。首先，我们需要理解问题的核心：“哪里是鹰安全的地方？”接着逐一分析各个选项：\\n\\npine树：pine树通常位于森林中，而森林中的环境并不总是适合鹰生存'\n",
      "Sampling T=1.0: 'e?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: ㅠ'\n",
      "Nucleus sampling: 'wildlife refuge\\n(e) sky\\nA: 通假字题型是一种考查学生对文言文中古今异义现象的辨析能力的题目。解答这类题目，首先要知道什么是通假字，其次要明确“通”和“假”的不同含义。“通”是'\n",
      "\n",
      "=== LOGITS ANALYSIS ===\n",
      "Logits shape: torch.Size([151936])\n",
      "Logits range: [-23.18, 14.04]\n",
      "Logits mean: 0.36\n",
      "Logits std: 3.27\n",
      "Max special token logit: 10.03\n",
      "Max regular token logit: 14.04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def diagnose_generation_issue(model, tokenizer):\n",
    "    \"\"\"Diagnose why generation is stopping early\"\"\"\n",
    "    \n",
    "    sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "    \n",
    "    print(\"=== TOKENIZER DIAGNOSIS ===\")\n",
    "    print(f\"Vocab size: {len(tokenizer)}\")\n",
    "    print(f\"Model vocab size: {model.config.vocab_size if hasattr(model.config, 'vocab_size') else 'Unknown'}\")\n",
    "    print(f\"PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    print(f\"EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "    print(f\"BOS token: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "    print(f\"UNK token: {tokenizer.unk_token} (ID: {tokenizer.unk_token_id})\")\n",
    "    \n",
    "    # Check if special tokens are properly set\n",
    "    if tokenizer.pad_token is None:\n",
    "        print(\"WARNING: pad_token is None!\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(f\"Set pad_token to eos_token: {tokenizer.pad_token}\")\n",
    "    \n",
    "    print(f\"\\n=== INPUT ANALYSIS ===\")\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids.to(device)\n",
    "    print(f\"Input length: {input_ids.shape[1]}\")\n",
    "    print(f\"Input tokens: {input_ids[0].tolist()}\")\n",
    "    print(f\"Decoded input: {repr(tokenizer.decode(input_ids[0]))}\")\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        print(f\"\\n=== STEP-BY-STEP GENERATION ===\")\n",
    "        \n",
    "        # Method 1: Manual step-by-step generation\n",
    "        current_ids = input_ids.clone()\n",
    "        for step in range(10):  # Generate 10 tokens max\n",
    "            outputs = model(current_ids)\n",
    "            logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "            \n",
    "            # Get top 5 most likely next tokens\n",
    "            top_k = torch.topk(logits, 5)\n",
    "            print(f\"Step {step + 1}:\")\n",
    "            print(f\"  Top 5 tokens: {[tokenizer.decode([idx]) for idx in top_k.indices]}\")\n",
    "            print(f\"  Top 5 probs: {torch.softmax(top_k.values, dim=0).tolist()}\")\n",
    "            \n",
    "            # Sample next token with temperature\n",
    "            probs = torch.softmax(logits / 0.7, dim=0)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            next_token_str = tokenizer.decode([next_token.item()])\n",
    "            print(f\"  Sampled: '{next_token_str}' (ID: {next_token.item()})\")\n",
    "            \n",
    "            # Check if it's a special token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                print(f\"  -> EOS token generated, stopping\")\n",
    "                break\n",
    "            if next_token.item() == tokenizer.pad_token_id:\n",
    "                print(f\"  -> PAD token generated, stopping\")\n",
    "                break\n",
    "                \n",
    "            current_ids = torch.cat([current_ids, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "        final_output = tokenizer.decode(current_ids[0], skip_special_tokens=True)\n",
    "        print(f\"\\nManual generation result: {repr(final_output)}\")\n",
    "        \n",
    "        print(f\"\\n=== DIFFERENT GENERATION METHODS ===\")\n",
    "        \n",
    "        # Method 2: model.generate with different settings\n",
    "        test_cases = [\n",
    "            {\"max_new_tokens\": 50, \"do_sample\": False, \"description\": \"Greedy\"},\n",
    "            {\"max_new_tokens\": 50, \"do_sample\": True, \"temperature\": 0.7, \"description\": \"Sampling T=0.7\"},\n",
    "            {\"max_new_tokens\": 50, \"do_sample\": True, \"temperature\": 1.0, \"description\": \"Sampling T=1.0\"},\n",
    "            {\"max_new_tokens\": 50, \"do_sample\": True, \"top_p\": 0.9, \"temperature\": 0.7, \"description\": \"Nucleus sampling\"},\n",
    "        ]\n",
    "        \n",
    "        for i, params in enumerate(test_cases):\n",
    "            description = params.pop(\"description\")\n",
    "            try:\n",
    "                outputs = model.generate(\n",
    "                    input_ids,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    **params\n",
    "                )\n",
    "                result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                print(f\"{description}: {repr(result[-100:])}\")  # Show last 100 chars\n",
    "            except Exception as e:\n",
    "                print(f\"{description}: ERROR - {e}\")\n",
    "        \n",
    "        print(f\"\\n=== LOGITS ANALYSIS ===\")\n",
    "        # Check if logits are reasonable\n",
    "        outputs = model(input_ids)\n",
    "        logits = outputs.logits[0, -1, :]\n",
    "        print(f\"Logits shape: {logits.shape}\")\n",
    "        print(f\"Logits range: [{logits.min().item():.2f}, {logits.max().item():.2f}]\")\n",
    "        print(f\"Logits mean: {logits.mean().item():.2f}\")\n",
    "        print(f\"Logits std: {logits.std().item():.2f}\")\n",
    "        \n",
    "        # Check for unusual patterns\n",
    "        if torch.isnan(logits).any():\n",
    "            print(\"WARNING: NaN values in logits!\")\n",
    "        if torch.isinf(logits).any():\n",
    "            print(\"WARNING: Infinite values in logits!\")\n",
    "            \n",
    "        # Check if model is generating mostly special tokens\n",
    "        special_token_ids = [tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id, tokenizer.unk_token_id]\n",
    "        special_token_ids = [tid for tid in special_token_ids if tid is not None]\n",
    "        \n",
    "        if special_token_ids:\n",
    "            special_logits = logits[special_token_ids]\n",
    "            regular_logits = logits.clone()\n",
    "            regular_logits[special_token_ids] = float('-inf')\n",
    "            \n",
    "            print(f\"Max special token logit: {special_logits.max().item():.2f}\")\n",
    "            print(f\"Max regular token logit: {regular_logits.max().item():.2f}\")\n",
    "            \n",
    "            if special_logits.max() > regular_logits.max():\n",
    "                print(\"WARNING: Special tokens have higher probability than regular tokens!\")\n",
    "\n",
    "# Run the diagnostic\n",
    "diagnose_generation_issue(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd2f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing datasets...\n",
      "\n",
      "==================================================\n",
      "DEBUGGING GENERATION BEFORE TRAINING\n",
      "==================================================\n",
      "\n",
      "=== DEBUGGING GENERATION ===\n",
      "Temperature: 0.7\n",
      "Max new tokens: 200\n",
      "Input prompt: <|im_start|>system\n",
      "You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with provi...\n",
      "Prompt token length: 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original generation (temp=0.7):\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Greedy generation:\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Low temp generation (0.1):\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Repetition check:\n",
      "Is repetitive: False\n",
      "\n",
      "Current generation parameters:\n",
      "TEMP: 0.7\n",
      "MAX_NEW_TOKENS: 200\n",
      "Model vocab size: 151936\n",
      "Tokenizer vocab size: 151665\n"
     ]
    }
   ],
   "source": [
    "# Debug generation\n",
    "import re\n",
    "\n",
    "def debug_generation(model, tokenizer, prompt_str, max_new_tokens=MAX_NEW_TOKENS, temp=TEMP):\n",
    "    \"\"\"Debug helper to inspect generation step by step\"\"\"\n",
    "    print(f\"\\n=== DEBUGGING GENERATION ===\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"Max new tokens: {max_new_tokens}\")\n",
    "    print(f\"Input prompt: {prompt_str[:200]}...\")\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(f\"Prompt token length: {prompt_ids.shape[1]}\")\n",
    "    \n",
    "    # Generate with different settings to compare\n",
    "    with torch.no_grad():\n",
    "        # Original generation\n",
    "        gen_ids = sample_no_grad(model, prompt_ids, max_new_tokens=max_new_tokens, temp=temp)\n",
    "        gen_str = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Alternative: greedy decoding\n",
    "        greedy_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens, \n",
    "                                   do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        greedy_str = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Alternative: low temperature\n",
    "        low_temp_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens,\n",
    "                                     do_sample=True, temperature=0.1, \n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "        low_temp_str = tokenizer.decode(low_temp_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nOriginal generation (temp={temp}):\")\n",
    "    print(repr(gen_str[-500:]))  # Show last 500 chars with escape sequences\n",
    "    print(f\"\\nGreedy generation:\")\n",
    "    print(repr(greedy_str[-500:]))\n",
    "    print(f\"\\nLow temp generation (0.1):\")\n",
    "    print(repr(low_temp_str[-500:]))\n",
    "    \n",
    "    # Check for repetitive patterns\n",
    "    def check_repetition(text):\n",
    "        # Look for repeating character sequences\n",
    "        for length in [2, 3, 4, 5]:\n",
    "            pattern = r'(.{' + str(length) + r'})\\1{10,}'  # Same sequence repeated 10+ times\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                print(f\"Found repetitive {length}-char pattern: '{matches[0]}'\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nRepetition check:\")\n",
    "    is_repetitive = check_repetition(gen_str)\n",
    "    print(f\"Is repetitive: {is_repetitive}\")\n",
    "    \n",
    "    return gen_str, greedy_str, low_temp_str\n",
    "\n",
    "# Modified training loop with debugging\n",
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Pre-process datasets\n",
    "print(\"Pre-processing datasets...\")\n",
    "train_batches = list(get_batches(train_dataset, parser, BATCH_SIZE))\n",
    "val_batches = list(get_batches(val_dataset, parser, BATCH_SIZE))\n",
    "\n",
    "# DEBUG: Test generation before training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEBUGGING GENERATION BEFORE TRAINING\")\n",
    "print(\"=\"*50)\n",
    "sample_prompt = train_batches[0][0][0]  # First prompt from first batch\n",
    "debug_generation(model, tokenizer, sample_prompt)\n",
    "\n",
    "# Add generation parameter validation\n",
    "print(f\"\\nCurrent generation parameters:\")\n",
    "print(f\"TEMP: {TEMP}\")\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "224r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
