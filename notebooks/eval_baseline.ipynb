{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/miniconda/envs/224r/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "TEMP = 0.7\n",
    "MAX_NEW_TOKENS = 200\n",
    "BATCH_SIZE = 4          # ← pick a batch size that fits in your GPU memory\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:902: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/sanjay/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "NAME = \"Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "#Qwen/Qwen3-1.7B\n",
    "MODEL_NAME = f\"Qwen/{NAME}\"\n",
    "\n",
    "# NAME = \"Llama-3.2-1B\"\n",
    "# MODEL_NAME = f\"meta-llama/{NAME}\"\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "# # model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     model.config.pad_token_id = model.config.eos_token_id\n",
    "# tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "NAME = \"Llama-3.2-3B-Instruct\"\n",
    "MODEL_NAME = f\"meta-llama/{NAME}\"\n",
    "\n",
    "\n",
    "# Set your Hugging Face token\n",
    "hf_token = \"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\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, use_auth_token=hf_token)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQAParser:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with providing a brief rationale for your answer, followed by the correct answer choice. For example:\n",
    "        \n",
    "Q: What do people use to absorb extra ink from a fountain pen?\n",
    "Answer Choices:\n",
    "(a) shirt pocket\n",
    "(b) calligrapher's hand\n",
    "(c) inkwell\n",
    "(d) desk drawer\n",
    "(e) blotter\n",
    "A: The answer must be used to absorb extra ink. Blotters are designed to absorb liquids. Therefore, the answer is blotter (e).\n",
    "\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices:\n",
    "(a) radio shack\n",
    "(b) substation\n",
    "(c) television\n",
    "(d) cabinet\n",
    "(e) desk\n",
    "A: The answer must require cable. Cable is used to provide satellite channels to televisions. Therefore, the answer is television (c).\n",
    "\n",
    "Format your answer in the same way, providing a BRIEF (<2-sentence) rationale followed by \"Therefore, the answer is *answer choice* (*letter label for answer choice*).\" Do not use any other format. If you are unsure, choose the most likely answer based on your reasoning.\n",
    "        \"\"\"\n",
    "\n",
    "        # self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with providing a brief rationale for your answer, followed by the correct answer choice. \"\"\"\n",
    "\n",
    "    def format_question(self, question_data):\n",
    "        q = question_data['question']\n",
    "        choices = \"\".join(f\"({lbl.lower()}) {txt}\\n\"\n",
    "                          for lbl, txt in zip(\n",
    "                              question_data['choices']['label'],\n",
    "                              question_data['choices']['text']\n",
    "                          ))\n",
    "        return f\"Q: {q}\\nAnswer Choices:\\n{choices.strip()}\\nA: \"\n",
    "\n",
    "    def format_prompt(self, question_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\",  \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\",    \"content\": self.format_question(question_data)}\n",
    "        ]\n",
    "        # `apply_chat_template` returns the tokenized prompt string + the raw question text.\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        ), messages[-1]['content']\n",
    "\n",
    "    def parse_llm_output(self, generated_text):\n",
    "        rationale = generated_text.removeprefix(\"</think>\").strip()\n",
    "        matches = re.findall(r\"\\(([a-e])\\)\", generated_text, re.IGNORECASE)\n",
    "        return rationale, (matches[-1].lower() if matches else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/150 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 1/150 [00:01<03:41,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏         | 2/150 [00:02<02:19,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 3/150 [00:02<01:44,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 4/150 [00:03<01:38,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 5/150 [00:03<01:37,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 6/150 [00:04<01:34,  1.52it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▍         | 7/150 [00:05<01:45,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 8/150 [00:06<01:46,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 9/150 [00:06<01:39,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 10/150 [00:07<01:39,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 11/150 [00:08<01:34,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 12/150 [00:08<01:31,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▊         | 13/150 [00:09<01:32,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 14/150 [00:09<01:24,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 15/150 [00:10<01:21,  1.65it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 16/150 [00:11<01:25,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█▏        | 17/150 [00:11<01:16,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 18/150 [00:12<01:14,  1.78it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 19/150 [00:12<01:10,  1.87it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 20/150 [00:13<01:15,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 21/150 [00:14<01:31,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▍        | 22/150 [00:15<01:38,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 23/150 [00:16<01:41,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 24/150 [00:16<01:43,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 25/150 [00:17<01:38,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 26/150 [00:18<01:36,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 27/150 [00:18<01:27,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▊        | 28/150 [00:19<01:31,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 29/150 [00:20<01:29,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 30/150 [00:21<01:30,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 31/150 [00:22<01:31,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██▏       | 32/150 [00:22<01:35,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 33/150 [00:23<01:31,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 34/150 [00:24<01:35,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 35/150 [00:25<01:26,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 36/150 [00:26<01:30,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▍       | 37/150 [00:26<01:25,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 38/150 [00:27<01:24,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 39/150 [00:28<01:24,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 40/150 [00:29<01:29,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 41/150 [00:30<01:37,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 42/150 [00:30<01:28,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▊       | 43/150 [00:31<01:25,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 44/150 [00:32<01:19,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 45/150 [00:32<01:10,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 46/150 [00:33<01:13,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███▏      | 47/150 [00:35<01:35,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 48/150 [00:35<01:25,  1.20it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 49/150 [00:36<01:22,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 50/150 [00:37<01:15,  1.32it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 51/150 [00:37<01:12,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▍      | 52/150 [00:38<01:14,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 53/150 [00:39<01:17,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 54/150 [00:40<01:11,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 55/150 [00:40<01:09,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 56/150 [00:41<01:07,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 57/150 [00:42<01:12,  1.28it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▊      | 58/150 [00:43<01:15,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 59/150 [00:44<01:17,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 60/150 [00:45<01:14,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 61/150 [00:45<01:14,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████▏     | 62/150 [00:46<01:10,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 63/150 [00:47<01:07,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 64/150 [00:48<01:05,  1.31it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 65/150 [00:48<01:00,  1.40it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 66/150 [00:49<01:03,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▍     | 67/150 [00:50<01:16,  1.08it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 68/150 [00:51<01:09,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 69/150 [00:52<01:16,  1.06it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 70/150 [00:53<01:08,  1.18it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 71/150 [00:54<01:04,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 72/150 [00:55<01:08,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▊     | 73/150 [00:55<01:08,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 74/150 [00:56<01:02,  1.21it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 75/150 [00:57<01:00,  1.24it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 76/150 [00:58<01:02,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████▏    | 77/150 [00:59<01:15,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 78/150 [01:00<01:09,  1.04it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 79/150 [01:01<01:03,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 80/150 [01:01<00:53,  1.30it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 81/150 [01:02<00:49,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 82/150 [01:02<00:43,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 83/150 [01:03<00:41,  1.63it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 84/150 [01:04<00:41,  1.61it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 85/150 [01:04<00:43,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 86/150 [01:05<00:39,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 87/150 [01:06<00:41,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▊    | 88/150 [01:06<00:41,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 89/150 [01:07<00:45,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 90/150 [01:08<00:44,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 91/150 [01:09<00:41,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████▏   | 92/150 [01:09<00:41,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 93/150 [01:10<00:40,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 94/150 [01:10<00:35,  1.57it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 95/150 [01:11<00:38,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 96/150 [01:12<00:37,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▍   | 97/150 [01:13<00:37,  1.43it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 98/150 [01:14<00:41,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 99/150 [01:14<00:38,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 100/150 [01:15<00:39,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 101/150 [01:16<00:33,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 102/150 [01:16<00:30,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▊   | 103/150 [01:17<00:34,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 104/150 [01:18<00:33,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 105/150 [01:18<00:30,  1.47it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 106/150 [01:19<00:31,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████▏  | 107/150 [01:20<00:31,  1.34it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 108/150 [01:21<00:33,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 109/150 [01:22<00:36,  1.14it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 110/150 [01:23<00:31,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 111/150 [01:24<00:35,  1.10it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▍  | 112/150 [01:25<00:33,  1.15it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 113/150 [01:25<00:31,  1.16it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 114/150 [01:26<00:30,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 115/150 [01:27<00:27,  1.29it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 116/150 [01:28<00:25,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 117/150 [01:28<00:24,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▊  | 118/150 [01:29<00:23,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 119/150 [01:30<00:22,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 120/150 [01:31<00:23,  1.26it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 121/150 [01:31<00:20,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████▏ | 122/150 [01:32<00:19,  1.46it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 123/150 [01:32<00:17,  1.54it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 124/150 [01:33<00:16,  1.62it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 125/150 [01:34<00:16,  1.48it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 126/150 [01:34<00:14,  1.60it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▍ | 127/150 [01:35<00:14,  1.55it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 128/150 [01:36<00:14,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 129/150 [01:36<00:14,  1.50it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 130/150 [01:37<00:13,  1.44it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 131/150 [01:38<00:12,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 132/150 [01:39<00:13,  1.36it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▊ | 133/150 [01:39<00:11,  1.51it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 134/150 [01:40<00:11,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 135/150 [01:41<00:10,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 136/150 [01:41<00:09,  1.42it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████▏| 137/150 [01:42<00:09,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 138/150 [01:43<00:08,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 139/150 [01:43<00:07,  1.49it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 140/150 [01:44<00:07,  1.38it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 141/150 [01:45<00:06,  1.39it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▍| 142/150 [01:45<00:05,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 143/150 [01:46<00:04,  1.56it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 144/150 [01:47<00:04,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 145/150 [01:48<00:04,  1.22it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 146/150 [01:49<00:03,  1.27it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 147/150 [01:49<00:02,  1.37it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▊| 148/150 [01:50<00:01,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 149/150 [01:51<00:00,  1.33it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 150/150 [01:52<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# dataset = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
    "import json\n",
    "import os \n",
    "\n",
    "VAL_SIZE=150\n",
    "dataset = load_dataset(\"commonsense_qa\", split=f\"validation[:{VAL_SIZE}]\")\n",
    "\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "logging = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    # Format the prompt for this question\n",
    "    prompt, _ = parser.format_prompt(example)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=TEMP,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Parse predicted answer\n",
    "    rationale, predicted_choice = parser.parse_llm_output(generated_text)\n",
    "    correct_answer = example[\"answerKey\"].lower()\n",
    "    if predicted_choice == correct_answer:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    log = {\n",
    "        \"prompt\": prompt,\n",
    "        \"score\": score,\n",
    "        \"rationale\": rationale,\n",
    "        \"predicted_choice\": predicted_choice,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"generated_text\": generated_text,\n",
    "    }\n",
    "    logging.append(log)\n",
    "    with open(f\"{NAME}_baseline_commonsenseqa.jsonl\", \"a\") as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "        f.write(\"\\n\")\n",
    "    # filename = \"Qwen3_1_7B_baseline_commonsenseqa.json\"\n",
    "    # with open(filename, \"a\") as f:\n",
    "    #     json.dump(log, f, indent=2)\n",
    "    #     f.write(\",\\n\")  # separate entries with commas\n",
    "\n",
    "\n",
    "    # Compare prediction to gold\n",
    "    if predicted_choice == correct_answer:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "224r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
