{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4834cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "TEMP = 0.7\n",
    "MAX_NEW_TOKENS = 200\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_SIZE = 5000\n",
    "VAL_SIZE = 500\n",
    "LOG_EVERY = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOG_JSON_DIR = \"batch_logs\"\n",
    "os.makedirs(LOG_JSON_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f168693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-1.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-1.7B\").to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abcfdb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQAParser:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with providing a brief rationale for your answer, followed by the correct answer choice. For example:\n",
    "        \n",
    "Q: What do people use to absorb extra ink from a fountain pen?\n",
    "Answer Choices:\n",
    "(a) shirt pocket\n",
    "(b) calligrapher's hand\n",
    "(c) inkwell\n",
    "(d) desk drawer\n",
    "(e) blotter\n",
    "A: The answer must be used to absorb extra ink. Blotters are designed to absorb liquids. Therefore, the answer is blotter (e).\n",
    "\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices:\n",
    "(a) radio shack\n",
    "(b) substation\n",
    "(c) television\n",
    "(d) cabinet\n",
    "(e) desk\n",
    "A: The answer must require cable. Cable is used to provide satellite channels to televisions. Therefore, the answer is television (c).\n",
    "\n",
    "Format your answer as:\n",
    "\"<BRIEF 1-2 sentence rationale>. Therefore, the answer is <answer text> (<answer letter choice>).\"\n",
    "\n",
    "Do not use any other format. If unsure, choose the most likely answer based on your reasoning.\n",
    "        \"\"\"\n",
    "\n",
    "    def format_question(self, question_data):\n",
    "        q = question_data['question']\n",
    "        choices = \"\".join(\n",
    "            f\"({lbl.lower()}) {txt}\\n\"\n",
    "            for lbl, txt in zip(\n",
    "                question_data['choices']['label'], question_data['choices']['text']\n",
    "            )\n",
    "        )\n",
    "        return f\"Q: {q}\\nAnswer Choices:\\n{choices.strip()}\\nA: \"\n",
    "\n",
    "    def format_prompt(self, question_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\",  \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\",    \"content\": self.format_question(question_data)}\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        ), messages[-1]['content']\n",
    "\n",
    "    def parse_llm_output(self, generated_text):\n",
    "        rationale = generated_text.removeprefix(\"</think>\").strip()\n",
    "        matches = re.findall(r\"\\(([a-e])\\)\", generated_text, re.IGNORECASE)\n",
    "        letter = matches[-1].lower() if matches else None\n",
    "        return rationale, letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1bc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_no_grad(model, batch_prompt_ids, max_new_tokens=MAX_NEW_TOKENS, temp=TEMP):\n",
    "    # `batch_prompt_ids` is shape (B, T)\n",
    "    seq = model.generate(\n",
    "        batch_prompt_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    # We return only the newly generated portion:\n",
    "    return seq[:, batch_prompt_ids.size(1):]  # shape (B, new_T)\n",
    "\n",
    "def compute_logprobs(model, batch_prompt_ids, batch_gen_ids, temp=TEMP):\n",
    "    # batch_prompt_ids: (B, T)\n",
    "    # batch_gen_ids:    (B, new_T)\n",
    "    full_ids = torch.cat([batch_prompt_ids, batch_gen_ids], dim=1)  # (B, T+new_T)\n",
    "    full_logits = model(full_ids).logits / temp  # (B, T+new_T, V)\n",
    "    full_logprobs = F.log_softmax(full_logits, dim=-1) # (B, T+new_T, V)\n",
    "\n",
    "    # log-probs of actual tokens (i.e., log P(token_t | token_<t))\n",
    "    token_logprobs = full_logprobs[:, :-1, :].gather(\n",
    "        2, full_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T_total - 1)\n",
    "\n",
    "    prompt_lens = (batch_prompt_ids != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1) # (B,)\n",
    "\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1) # (B,)\n",
    "    logprobs = []\n",
    "    for i in range(batch_prompt_ids.size(0)):\n",
    "        start = int(prompt_lens[i].item()) - 1\n",
    "        gen_len = int(gen_lens[i].item())\n",
    "        logprobs.append(token_logprobs[i, start : start + gen_len].sum())\n",
    "\n",
    "    return torch.stack(logprobs, dim=0)  # (B,)\n",
    "\n",
    "def compute_binary_reward(final_answer, correct_answer, question=None, rationale=None):\n",
    "    return 1.0 if final_answer == correct_answer else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65fc1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset, parser, batch_size):\n",
    "    # Returns (prompt_strs, raw_qs, correct_answers), each a len-B List[str].\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_dict = dataset[i : i + batch_size]\n",
    "        batch_items = [\n",
    "            {key: batch_dict[key][i] for key in batch_dict}\n",
    "            for i in range(len(batch_dict[\"id\"]))\n",
    "        ]\n",
    "        prompt_strs, raw_qs, correct_keys = [], [], []\n",
    "        for item in batch_items:\n",
    "            p_str, raw_q = parser.format_prompt(item)\n",
    "            prompt_strs.append(p_str)\n",
    "            raw_qs.append(raw_q)\n",
    "            correct_keys.append(item[\"answerKey\"].lower())\n",
    "        yield prompt_strs, raw_qs, correct_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   1%|          | 13/1250 [01:04<2:17:32,  6.67s/it]"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    batch_count = 0\n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        get_batches(train_dataset, parser, BATCH_SIZE),\n",
    "        desc=f\"Training Epoch {epoch + 1}\", total=TRAIN_SIZE // BATCH_SIZE\n",
    "    )):\n",
    "        # tokenize and left-pad prompt strings\n",
    "        prompt_ids = tokenizer(prompt_strs, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=512\n",
    "        ).to(device)[\"input_ids\"] # (B, T)\n",
    "        \n",
    "        # sample responses (no grad)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = sample_no_grad(\n",
    "                model, prompt_ids,\n",
    "                max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "            )  # (B, new_T)\n",
    "            \n",
    "        # decode and parse generated outputs\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True) # len-B List[str]\n",
    "        rationales, pred_answers = zip( # each a len-B List[str]\n",
    "            *[parser.parse_llm_output(gen_str) for gen_str in gen_strs]\n",
    "        )\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)  # (B,)\n",
    "        rewards = torch.tensor([\n",
    "            compute_binary_reward(ans, corr)\n",
    "            for ans, corr in zip(pred_answers, correct_answers)\n",
    "        ], device=device)\n",
    "        loss = -(rewards * logprobs).mean()  # (B,)\n",
    "        if batch_count % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), batch_count)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), batch_count)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), batch_count)\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_count,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_count}.json\")\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        batch_count += 1\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### VALIDATION LOGGING\n",
    "        if batch_count % (LOG_EVERY * 5) == 0:\n",
    "            model.eval()\n",
    "            val_rewards_all, val_logprobs_all, val_losses_all = [], [], []\n",
    "            collected_examples = []\n",
    "\n",
    "            for val_prompt_strs, val_raw_qs, val_correct_answers in get_batches(val_dataset, parser, BATCH_SIZE):\n",
    "                val_prompt_ids = tokenizer(val_prompt_strs, return_tensors=\"pt\",\n",
    "                    padding=True, truncation=True, max_length=512\n",
    "                ).to(device)[\"input_ids\"]\n",
    "\n",
    "                val_gen_ids = sample_no_grad(model, val_prompt_ids)\n",
    "                val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                val_rationales, val_pred_answers = zip(\n",
    "                    *[parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                )\n",
    "                val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids)\n",
    "                val_rwds = torch.tensor([\n",
    "                    compute_binary_reward(ans, corr)\n",
    "                    for ans, corr in zip(val_pred_answers, val_correct_answers)\n",
    "                ], device=device)\n",
    "\n",
    "                val_loss = -(val_rwds * val_logp).mean()\n",
    "\n",
    "                val_rewards_all.append(val_rwds)\n",
    "                val_logprobs_all.append(val_logp)\n",
    "                val_losses_all.append(val_loss)\n",
    "\n",
    "                for q, r, a, p, rew, lp in zip(\n",
    "                    val_raw_qs, val_rationales, val_correct_answers, val_pred_answers, val_rwds, val_logp\n",
    "                ):\n",
    "                    collected_examples.append({\n",
    "                        \"question\": q,\n",
    "                        \"rationale\": r,\n",
    "                        \"predicted_answer\": p if p else \"None\",\n",
    "                        \"correct_answer\": a,\n",
    "                        \"reward\": float(rew.item()),\n",
    "                        \"logprob\": float(lp.item())\n",
    "                    })\n",
    "\n",
    "            writer.add_scalar(\"Eval/AvgReward\", torch.cat(val_rewards_all).mean().item(), batch_count)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", torch.cat(val_logprobs_all).mean().item(), batch_count)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", torch.stack(val_losses_all).mean().item(), batch_count)\n",
    "\n",
    "            # Random sample of examples\n",
    "            log_sample = random.sample(collected_examples, k=5)\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_count,\n",
    "                \"questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_count}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "\n",
    "            model.train()\n",
    "\n",
    "\n",
    "    avg_loss = train_loss / batch_count\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2f37f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
