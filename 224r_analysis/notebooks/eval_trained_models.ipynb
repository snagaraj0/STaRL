{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'exp2_commonqa_llama_baseline_mean'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DEFINE MODEL PATH HERE\n",
    "LOCAL_MODEL_PATH = \"/home/sanjay/224r/results/exp2_commonqa_llama_baseline_mean/checkpoints/epoch1_batch1500\"\n",
    "EXP_NAME = LOCAL_MODEL_PATH.split(\"/\")[-3]\n",
    "EXP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/miniconda/envs/224r/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "TEMP = 0.7\n",
    "MAX_NEW_TOKENS = 200\n",
    "BATCH_SIZE = 4          # ← pick a batch size that fits in your GPU memory\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load tokenizer and model from local checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(LOCAL_MODEL_PATH).to(device)\n",
    "\n",
    "class CommonsenseQAParser:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with providing a brief rationale for your answer, followed by the correct answer choice. For example:\n",
    "        \n",
    "Q: What do people use to absorb extra ink from a fountain pen?\n",
    "Answer Choices:\n",
    "(a) shirt pocket\n",
    "(b) calligrapher's hand\n",
    "(c) inkwell\n",
    "(d) desk drawer\n",
    "(e) blotter\n",
    "A: The answer must be used to absorb extra ink. Blotters are designed to absorb liquids. Therefore, the answer is blotter (e).\n",
    "\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices:\n",
    "(a) radio shack\n",
    "(b) substation\n",
    "(c) television\n",
    "(d) cabinet\n",
    "(e) desk\n",
    "A: The answer must require cable. Cable is used to provide satellite channels to televisions. Therefore, the answer is television (c).\n",
    "\n",
    "Format your answer in the same way, providing a BRIEF (<2-sentence) rationale followed by \"Therefore, the answer is *answer choice* (*letter label for answer choice*).\" Do not use any other format. If you are unsure, choose the most likely answer based on your reasoning.\n",
    "        \"\"\"\n",
    "\n",
    "        # self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with providing a brief rationale for your answer, followed by the correct answer choice. \"\"\"\n",
    "\n",
    "    def format_question(self, question_data):\n",
    "        q = question_data['question']\n",
    "        choices = \"\".join(f\"({lbl.lower()}) {txt}\\n\"\n",
    "                          for lbl, txt in zip(\n",
    "                              question_data['choices']['label'],\n",
    "                              question_data['choices']['text']\n",
    "                          ))\n",
    "        return f\"Q: {q}\\nAnswer Choices:\\n{choices.strip()}\\nA: \"\n",
    "\n",
    "    def format_prompt(self, question_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\",  \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\",    \"content\": self.format_question(question_data)}\n",
    "        ]\n",
    "        # `apply_chat_template` returns the tokenized prompt string + the raw question text.\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        ), messages[-1]['content']\n",
    "\n",
    "    def parse_llm_output(self, generated_text):\n",
    "        rationale = generated_text.removeprefix(\"</think>\").strip()\n",
    "        matches = re.findall(r\"\\(([a-e])\\)\", generated_text, re.IGNORECASE)\n",
    "        return rationale, (matches[-1].lower() if matches else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"commonsense_qa\", split=\"validation\")\n",
    "import json\n",
    "import os \n",
    "\n",
    "VAL_SIZE=150\n",
    "dataset = load_dataset(\"commonsense_qa\", split=f\"validation[:{VAL_SIZE}]\")\n",
    "\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "logging = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    # Format the prompt for this question\n",
    "    prompt, _ = parser.format_prompt(example)\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=True, truncation=True).to(device)\n",
    "\n",
    "    # Generate response\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,\n",
    "        temperature=TEMP,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Parse predicted answer\n",
    "    rationale, predicted_choice = parser.parse_llm_output(generated_text)\n",
    "    correct_answer = example[\"answerKey\"].lower()\n",
    "    if predicted_choice == correct_answer:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    log = {\n",
    "        \"prompt\": prompt,\n",
    "        \"score\": score,\n",
    "        \"rationale\": rationale,\n",
    "        \"predicted_choice\": predicted_choice,\n",
    "        \"correct_answer\": correct_answer,\n",
    "        \"generated_text\": generated_text,\n",
    "    }\n",
    "    logging.append(log)\n",
    "    with open(f\"{EXP_NAME}_commonsenseqa.jsonl\", \"a\") as f:\n",
    "        json.dump(log, f, indent=2)\n",
    "        f.write(\"\\n\")\n",
    "    # filename = \"Qwen3_1_7B_baseline_commonsenseqa.json\"\n",
    "    # with open(filename, \"a\") as f:\n",
    "    #     json.dump(log, f, indent=2)\n",
    "    #     f.write(\",\\n\")  # separate entries with commas\n",
    "\n",
    "\n",
    "    # Compare prediction to gold\n",
    "    if predicted_choice == correct_answer:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "# Final accuracy\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "224r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
