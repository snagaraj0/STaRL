{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4834cffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanjay/miniconda/envs/224r/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "TEMP = 0.7\n",
    "MAX_NEW_TOKENS = 256\n",
    "BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 5\n",
    "TRAIN_SIZE = 5000\n",
    "VAL_SIZE = 500\n",
    "LOG_EVERY = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LOG_JSON_DIR = \"batch_logs_test\"\n",
    "os.makedirs(LOG_JSON_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(LOG_JSON_DIR, \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f168693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\" #\"Qwen/Qwen2.5-1.5B-Instruct\" #\"Qwen/Qwen3-1.7B\" #\"Qwen/Qwen2.5-1.5B-Instruct\" #Qwen/Qwen3-1.7B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #use_auth_token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device) #use_auth_token=hf_token).to(device)\n",
    "if tokenizer.pad_token is None:\n",
    "    #tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    tokenizer.pad_token = \"<|reserved_special_token_4|>\"\n",
    "    # tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147ea609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|reserved_special_token_4|>'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8a7efb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (d)  A wildlife refuge provides a safe habitat for bald eagles. They have nesting trees and access to fish and other food sources. The other choices are not safe places for bald eagles. Pine trees are their nesting trees, but they do not provide a safe habitat. Open country is not suitable for bald eagles, as they are adapted to living near water. The sky is not a safe place for bald eagles, as they need to land and nest in trees.\n",
      "The best answer is D.\n"
     ]
    }
   ],
   "source": [
    "# Test base model generation quality \n",
    "model.eval()\n",
    "sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_type_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abcfdb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommonsenseQAParser:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.system_prompt = \"\"\"You are an expert at applying commonsense reasoning to answer multiple-choice questions. \n",
    "\n",
    "For each question:\n",
    "1. Identify what the question is asking for\n",
    "2. Evaluate each choice against the question's requirements\n",
    "3. Select the choice that best fits\n",
    "\n",
    "Examples:\n",
    "\n",
    "Q: What do people use to absorb extra ink from a fountain pen?\n",
    "Answer Choices: (a) shirt pocket (b) calligrapher's hand (c) inkwell (d) desk drawer (e) blotter\n",
    "A: The question asks for something that absorbs ink. A blotter is specifically designed to absorb excess ink from writing instruments. Therefore, the answer is blotter (e).\n",
    "\n",
    "Q: What home entertainment equipment requires cable?\n",
    "Answer Choices: (a) radio shack (b) substation (c) television (d) cabinet (e) desk\n",
    "A: The question asks for entertainment equipment that needs cable service. Televisions can receive cable TV signals for entertainment programming. Therefore, the answer is television (c).\n",
    "\n",
    "Always format your response as:\n",
    "\"<Clear reasoning in a sentence or two>. Therefore, the answer is <answer text> (<letter>). Choose the most reasonable answer if uncertain.\"\"\"\n",
    "    def format_question(self, question_data):\n",
    "        q = question_data['question']\n",
    "        choices = \"\".join(\n",
    "            f\"({lbl.lower()}) {txt}\\n\"\n",
    "            for lbl, txt in zip(\n",
    "                question_data['choices']['label'], question_data['choices']['text']\n",
    "            )\n",
    "        )\n",
    "        return f\"Q: {q}\\nAnswer Choices:\\n{choices.strip()}\\nA: \"\n",
    "\n",
    "    def format_prompt(self, question_data):\n",
    "        messages = [\n",
    "            {\"role\": \"system\",  \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\",    \"content\": self.format_question(question_data)}\n",
    "        ]\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "            enable_thinking=False\n",
    "        ), messages[-1]['content']\n",
    "\n",
    "    def parse_llm_output(self, generated_text):\n",
    "        rationale = generated_text.removeprefix(\"</think>\").strip()\n",
    "        matches = re.findall(r\"\\(([a-e])\\)\", generated_text, re.IGNORECASE)\n",
    "        letter = matches[-1].lower() if matches else None\n",
    "        return rationale, letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b1bc2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample_no_grad(model, batch_prompt_ids, max_new_tokens=MAX_NEW_TOKENS, temp=TEMP):\n",
    "    # `batch_prompt_ids` is shape (B, T)\n",
    "    seq = model.generate(\n",
    "        batch_prompt_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temp,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.2,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    # We return only the newly generated portion\n",
    "    return seq[:, batch_prompt_ids.size(1):]  # shape (B, new_T)\n",
    "\n",
    "def compute_logprobs(model, batch_prompt_ids, batch_gen_ids, temp=TEMP):\n",
    "    \"\"\"\n",
    "    # batch_prompt_ids: (B, T)\n",
    "    # batch_gen_ids:    (B, new_T)\n",
    "    full_ids = torch.cat([batch_prompt_ids, batch_gen_ids], dim=1)  # (B, T+new_T)\n",
    "    full_logits = model(full_ids).logits / temp  # (B, T+new_T, V)\n",
    "    full_logprobs = F.log_softmax(full_logits, dim=-1) # (B, T+new_T, V)\n",
    "\n",
    "    # log-probs of actual tokens (i.e., log P(token_t | token_<t))\n",
    "    token_logprobs = full_logprobs[:, :-1, :].gather(\n",
    "        2, full_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T_total - 1)\n",
    "\n",
    "    prompt_lens = (batch_prompt_ids != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1) # (B,)\n",
    "\n",
    "    logprobs = []\n",
    "    for i in range(batch_prompt_ids.size(0)):\n",
    "        start = int(prompt_lens[i].item()) - 1\n",
    "        gen_len = int(gen_lens[i].item())\n",
    "        logprobs.append(token_logprobs[i, start : start + gen_len].sum())\n",
    "\n",
    "    return torch.stack(logprobs, dim=0)  # (B,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Concatenate inputs\n",
    "    full_ids = torch.cat([batch_prompt_ids, batch_gen_ids], dim=1)  # (B, T+new_T)\n",
    "\n",
    "    # Forward pass\n",
    "    full_logits = model(full_ids).logits / temp  # (B, T+new_T, V)\n",
    "    full_logprobs = F.log_softmax(full_logits, dim=-1)  # (B, T+new_T, V)\n",
    "\n",
    "    # Get logprobs of actual tokens (predict token_t at position t)\n",
    "    token_logprobs = full_logprobs[:, :-1, :].gather(\n",
    "        2, full_ids[:, 1:].unsqueeze(-1)\n",
    "    ).squeeze(-1)  # (B, T+new_T - 1)\n",
    "\n",
    "    # Lengths\n",
    "    prompt_lens = (batch_prompt_ids != tokenizer.pad_token_id).sum(dim=1)  # (B,)\n",
    "    gen_lens = (batch_gen_ids != tokenizer.pad_token_id).sum(dim=1)        # (B,)\n",
    "\n",
    "    # Create a mask for the generated tokens\n",
    "    _, total_len = full_ids.size()\n",
    "    token_pos = torch.arange(total_len - 1, device=full_ids.device).unsqueeze(0)  # (1, T+new_T - 1)\n",
    "    gen_masks = (\n",
    "        (token_pos >= prompt_lens.unsqueeze(1)) &\n",
    "        (token_pos < (prompt_lens + gen_lens).unsqueeze(1))\n",
    "    )  # (B, T+new_T - 1), True where generated tokens live\n",
    "\n",
    "    # Mask and sum\n",
    "    gen_token_logprobs = token_logprobs * gen_masks  # (B, T+new_T - 1)\n",
    "    return gen_token_logprobs.sum(dim=1)  # (B,)\n",
    "\n",
    "def compute_binary_reward(final_answer, correct_answer, question=None, rationale=None):\n",
    "    return 1.0 if final_answer == correct_answer else -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65fc1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(dataset, parser, batch_size):\n",
    "    # Returns (prompt_strs, raw_qs, correct_answers), each a len-B List[str].\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_dict = dataset[i : i + batch_size]\n",
    "        batch_items = [\n",
    "            {key: batch_dict[key][i] for key in batch_dict}\n",
    "            for i in range(len(batch_dict[\"id\"]))\n",
    "        ]\n",
    "        prompt_strs, raw_qs, correct_keys = [], [], []\n",
    "        for item in batch_items:\n",
    "            p_str, raw_q = parser.format_prompt(item)\n",
    "            prompt_strs.append(p_str)\n",
    "            raw_qs.append(raw_q)\n",
    "            correct_keys.append(item[\"answerKey\"].lower())\n",
    "        yield prompt_strs, raw_qs, correct_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5e442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def compute_LM_reward(model_name, correct_answer, question, rationale):\n",
    "    if \"gpt\" in model_name:\n",
    "        api_key = \"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\"\n",
    "        client = openai.OpenAI(api_key=api_key)\n",
    "    \n",
    "        prompt = f\"\"\"Given the below question, whose correct answer is ({correct_answer}), assign a numerical score to between -1 (poor) and 1 (excellent) to the given response. Score the response based on the quality of reasoning, logical coherence and consistency, and accuracy in answering the question.\n",
    "\n",
    "        Provide ONLY a decimal score between -1 (poor) and 1 (excellent), rounded to the nearest hundredth. Your answer should be \"X.XX\", without quotes or any explanation.\n",
    "\n",
    "        QUESTION: {question}\n",
    "        RESPONSE: {rationale}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\", messages=[{\"role\": \"user\", \"content\": prompt}], max_tokens=10\n",
    "            )\n",
    "\n",
    "            score_text = response.choices[0].message.content.strip()\n",
    "            print(score_text)\n",
    "            return float(score_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during API call: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27fe82",
   "metadata": {},
   "source": [
    "### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c03c2994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 50/1250 [03:21<1:20:46,  4.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 80\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m val_prompt_strs, val_raw_qs, val_correct_answers \u001b[38;5;129;01min\u001b[39;00m get_batches(val_dataset, parser, BATCH_SIZE):\n\u001b[1;32m     76\u001b[0m     val_prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer(val_prompt_strs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     77\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     78\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 80\u001b[0m     val_gen_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_no_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_prompt_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     val_gen_strs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(val_gen_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m     val_rationales, val_pred_answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;241m*\u001b[39m[parser\u001b[38;5;241m.\u001b[39mparse_llm_output(gen) \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m val_gen_strs]\n\u001b[1;32m     84\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m, in \u001b[0;36msample_no_grad\u001b[0;34m(model, batch_prompt_ids, max_new_tokens, temp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_no_grad\u001b[39m(model, batch_prompt_ids, max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS, temp\u001b[38;5;241m=\u001b[39mTEMP):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# `batch_prompt_ids` is shape (B, T)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# We return only the newly generated portion\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq[:, batch_prompt_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):]\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3564\u001b[0m     outputs,\n\u001b[1;32m   3565\u001b[0m     model_kwargs,\n\u001b[1;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:703\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    698\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    699\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    700\u001b[0m )\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:436\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    434\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 436\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:257\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:164\u001b[0m, in \u001b[0;36mQwen2Attention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    163\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 164\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:87\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001b[0m\n\u001b[1;32m     85\u001b[0m cos \u001b[38;5;241m=\u001b[39m cos\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[1;32m     86\u001b[0m sin \u001b[38;5;241m=\u001b[39m sin\u001b[38;5;241m.\u001b[39munsqueeze(unsqueeze_dim)\n\u001b[0;32m---> 87\u001b[0m q_embed \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (\u001b[43mrotate_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m     88\u001b[0m k_embed \u001b[38;5;241m=\u001b[39m (k \u001b[38;5;241m*\u001b[39m cos) \u001b[38;5;241m+\u001b[39m (rotate_half(k) \u001b[38;5;241m*\u001b[39m sin)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_embed, k_embed\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:62\u001b[0m, in \u001b[0;36mrotate_half\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     60\u001b[0m x1 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, : x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     61\u001b[0m x2 \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m :]\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "# Subsample validation (i.e. test) to 150 examples\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        get_batches(train_dataset, parser, BATCH_SIZE),\n",
    "        desc=f\"Training Epoch {epoch + 1}\", total=TRAIN_SIZE // BATCH_SIZE\n",
    "    )):\n",
    "        prompt_ids = tokenizer(prompt_strs, return_tensors=\"pt\",\n",
    "            padding=True, truncation=True, max_length=512\n",
    "        ).to(device)[\"input_ids\"]\n",
    "\n",
    "        gen_ids = sample_no_grad(\n",
    "            model, prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "        )\n",
    "\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        rationales, pred_answers = zip(*[\n",
    "            parser.parse_llm_output(gen_str) for gen_str in gen_strs\n",
    "        ])\n",
    "\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)  # (B,)\n",
    "        rewards = torch.tensor([\n",
    "            compute_binary_reward(ans, corr)\n",
    "            for ans, corr in zip(pred_answers, correct_answers)\n",
    "        ], device=device).float()\n",
    "        # Version 1 (-1 negative reward)\n",
    "        loss = -(rewards * logprobs).mean()\n",
    "        # Version 2 (-1 negative reward) with baseline\n",
    "        #baseline = rewards.mean()\n",
    "        #advantage = rewards - baseline\n",
    "\n",
    "        if batch_idx % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), batch_idx)\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ### VALIDATION LOGGING ###\n",
    "        if batch_idx % (LOG_EVERY * 5) == 0:\n",
    "            model.eval()\n",
    "            total_val_reward, total_val_logprob, total_val_loss = 0.0, 0.0, 0.0\n",
    "            num_val_examples = 0\n",
    "            collected_examples = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_prompt_strs, val_raw_qs, val_correct_answers in get_batches(val_dataset, parser, BATCH_SIZE):\n",
    "                    val_prompt_ids = tokenizer(val_prompt_strs, return_tensors=\"pt\",\n",
    "                        padding=True, truncation=True, max_length=512\n",
    "                    ).to(device)[\"input_ids\"]\n",
    "\n",
    "                    val_gen_ids = sample_no_grad(model, val_prompt_ids)\n",
    "                    val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                    val_rationales, val_pred_answers = zip(\n",
    "                        *[parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                    )\n",
    "                    val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids)\n",
    "                    val_rwds = torch.tensor([\n",
    "                        compute_binary_reward(ans, corr)\n",
    "                        for ans, corr in zip(val_pred_answers, val_correct_answers)\n",
    "                    ], device=device)\n",
    "\n",
    "                    val_loss = -(val_rwds * val_logp).mean()\n",
    "\n",
    "                    total_val_reward += val_rwds.sum().item()\n",
    "                    total_val_logprob += val_logp.sum().item()\n",
    "                    total_val_loss += val_loss.item() * val_rwds.size(0)\n",
    "                    num_val_examples += val_rwds.size(0)\n",
    "\n",
    "                    for q, r, a, p, rew, lp in zip(\n",
    "                        val_raw_qs, val_rationales, val_correct_answers, val_pred_answers, val_rwds, val_logp\n",
    "                    ):\n",
    "                        collected_examples.append({\n",
    "                            \"question\": q,\n",
    "                            \"rationale\": r,\n",
    "                            \"predicted_answer\": p if p else \"None\",\n",
    "                            \"correct_answer\": a,\n",
    "                            \"reward\": float(rew.item()),\n",
    "                            \"logprob\": float(lp.item())\n",
    "                        })\n",
    "\n",
    "                    # Explicit cleanup\n",
    "                    del val_prompt_ids, val_gen_ids, val_logp, val_rwds, val_loss\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "            writer.add_scalar(\"Eval/AvgReward\", total_val_reward / num_val_examples, batch_idx)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", total_val_logprob / num_val_examples, batch_idx)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", total_val_loss / num_val_examples, batch_idx)\n",
    "\n",
    "            log_sample = random.sample(collected_examples, k=5)\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "\n",
    "\n",
    "    avg_loss = train_loss / batch_idx\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce495e9",
   "metadata": {},
   "source": [
    "### Version 2: Optimize logging + batch size, optimize batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7e7ea85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing datasets...\n",
      "Created 10 training batches, 4 validation batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  50%|█████     | 5/10 [00:25<00:26,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running validation at batch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 10/10 [00:55<00:00,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg train loss per batch: -27.7505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  50%|█████     | 5/10 [00:16<00:15,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running validation at batch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 10/10 [00:43<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg train loss per batch: -7.1694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  50%|█████     | 5/10 [00:18<00:17,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running validation at batch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 10/10 [00:45<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg train loss per batch: -7.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4:  50%|█████     | 5/10 [00:17<00:17,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running validation at batch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 10/10 [00:41<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg train loss per batch: -11.3044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5:  50%|█████     | 5/10 [00:15<00:15,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running validation at batch 5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 10/10 [00:38<00:00,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 avg train loss per batch: -9.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LOG_EVERY = 50\n",
    "BATCH_SIZE = 5\n",
    "# Subsample training to overfit to 50 examples (original 5000)\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:50]\")\n",
    "# Subsample validation (i.e. test) to 20 examples (original 150)\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:20]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-7)\n",
    "\n",
    "# Pre-process datasets to avoid repeated parsing\n",
    "print(\"Pre-processing datasets...\")\n",
    "train_batches = list(get_batches(train_dataset, parser, BATCH_SIZE))\n",
    "val_batches = list(get_batches(val_dataset, parser, BATCH_SIZE))\n",
    "print(f\"Created {len(train_batches)} training batches, {len(val_batches)} validation batches\")\n",
    "\n",
    "# Reduce validation frequency significantly\n",
    "VALIDATION_FREQUENCY = len(train_batches) // 2  \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        train_batches, desc=f\"Training Epoch {epoch + 1}\"\n",
    "    )):\n",
    "        #print(prompt_strs)\n",
    "        #print(raw_qs)\n",
    "        #print(correct_answers)\n",
    "\n",
    "        # Tokenize once with proper device placement\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_strs, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).input_ids.to(device)\n",
    "        \n",
    "        # Generate samples\n",
    "        gen_ids = sample_no_grad(\n",
    "            model, prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "        )\n",
    "\n",
    "        # Batch decode and parse\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        parsed_outputs = [parser.parse_llm_output(gen_str) for gen_str in gen_strs]\n",
    "        rationales, pred_answers = zip(*parsed_outputs)\n",
    "        #print(rationales)\n",
    "        #print(pred_answers)\n",
    "\n",
    "        # Compute loss components\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)\n",
    "        rewards = torch.tensor([\n",
    "            compute_binary_reward(ans, corr)\n",
    "            for ans, corr in zip(pred_answers, correct_answers)\n",
    "        ], device=device, dtype=torch.float32)\n",
    "        \n",
    "        # REINFORCE loss\n",
    "        #loss = -(rewards * logprobs).mean()\n",
    "\n",
    "        # REINFORCE (baseline-normalized) loss\n",
    "        loss = -((rewards - rewards.mean()) * logprobs).mean()\n",
    "\n",
    "        # Gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "       \n",
    "        # Lightweight logging (reduced frequency)\n",
    "        if batch_idx % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            \n",
    "            # Simplified logging - only save one example per LOG_EVERY batches\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "        \n",
    "        # Aggressive memory cleanup\n",
    "        del prompt_ids, gen_ids, logprobs, rewards, loss\n",
    "        if batch_idx % 10 == 0:  # Clean cache every 10 batches\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \"\"\"\n",
    "        # Debug: Test model generation quality post training\n",
    "        model.eval()\n",
    "        sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "            outputs = model.generate(\n",
    "                    inputs.input_ids,\n",
    "                    max_new_tokens=1024,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "            result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            print(\"Generated:\", result)\n",
    "        \"\"\"\n",
    "        \n",
    "        #if batch_idx % 5 == 0:\n",
    "        #    break\n",
    "\n",
    "        ### VALIDATION LOGGING (Much less frequent) ###\n",
    "\n",
    "        if batch_idx > 0 and batch_idx % VALIDATION_FREQUENCY == 0:\n",
    "            print(f\"\\n--- Running validation at batch {batch_idx} ---\")\n",
    "            model.eval()\n",
    "            \n",
    "            val_rewards, val_logprobs, val_losses = [], [], []\n",
    "            collected_examples = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_batches:\n",
    "                    val_prompt_strs, val_raw_qs, val_correct_answers = val_batch\n",
    "                    \n",
    "                    val_prompt_ids = tokenizer(\n",
    "                        val_prompt_strs, \n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=512\n",
    "                    ).input_ids.to(device)\n",
    "\n",
    "                    val_gen_ids = sample_no_grad(model, val_prompt_ids, \n",
    "                                               max_new_tokens=MAX_NEW_TOKENS, temp=TEMP)\n",
    "                    val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                    \n",
    "                    val_parsed = [parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                    val_rationales, val_pred_answers = zip(*val_parsed)\n",
    "                    \n",
    "                    val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids, temp=TEMP)\n",
    "                    val_rwds = torch.tensor([\n",
    "                        compute_binary_reward(ans, corr)\n",
    "                        for ans, corr in zip(val_pred_answers, val_correct_answers)\n",
    "                    ], device=device, dtype=torch.float32)\n",
    "\n",
    "                    val_loss = -(val_rwds * val_logp).mean()\n",
    "\n",
    "                    # Collect metrics\n",
    "                    val_rewards.extend(val_rwds.cpu().tolist())\n",
    "                    val_logprobs.extend(val_logp.cpu().tolist())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # Collect sample examples (limit to reduce memory)\n",
    "                    for i, (q, r, a, p, rew, lp) in enumerate(zip(\n",
    "                        val_raw_qs[:2], val_rationales[:2], val_correct_answers[:2], \n",
    "                        val_pred_answers[:2], val_rwds[:2], val_logp[:2]\n",
    "                    )):\n",
    "                        collected_examples.append({\n",
    "                            \"question\": q,\n",
    "                            \"rationale\": r,\n",
    "                            \"predicted_answer\": p if p else \"None\",\n",
    "                            \"correct_answer\": a,\n",
    "                            \"reward\": float(rew.item()),\n",
    "                            \"logprob\": float(lp.item())\n",
    "                        })\n",
    "\n",
    "                    # Immediate cleanup\n",
    "                    del val_prompt_ids, val_gen_ids, val_logp, val_rwds, val_loss\n",
    "\n",
    "            # Log validation metrics\n",
    "            avg_val_reward = sum(val_rewards) / len(val_rewards)\n",
    "            avg_val_logprob = sum(val_logprobs) / len(val_logprobs)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            global_step = epoch * len(train_batches) + batch_idx\n",
    "            writer.add_scalar(\"Eval/AvgReward\", avg_val_reward, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", avg_val_logprob, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", avg_val_loss, global_step)\n",
    "\n",
    "            # Save validation examples (reduced sample size)\n",
    "            log_sample = random.sample(collected_examples, k=min(3, len(collected_examples)))\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"avg_reward\": avg_val_reward,\n",
    "                \"avg_logprob\": avg_val_logprob,\n",
    "                \"avg_loss\": avg_val_loss,\n",
    "                \"sample_questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"sample_rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"sample_predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"sample_correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"sample_rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"sample_logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "            \n",
    "            model.train()  # Return to training mode\n",
    "            torch.cuda.empty_cache()  # Clean up after validation\n",
    "          \n",
    "    avg_loss = train_loss / len(train_batches)\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    #writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032514de",
   "metadata": {},
   "source": [
    "### Version 3; LLM as judge for rationale evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989a52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing datasets...\n",
      "Created 25 training batches, 5 validation batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   4%|▍         | 1/25 [00:03<01:13,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   8%|▊         | 2/25 [00:05<01:05,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  12%|█▏        | 3/25 [00:08<01:00,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  16%|█▌        | 4/25 [00:10<00:52,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  20%|██        | 5/25 [00:12<00:47,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  24%|██▍       | 6/25 [00:16<00:52,  2.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  28%|██▊       | 7/25 [00:18<00:48,  2.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  32%|███▏      | 8/25 [00:20<00:43,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "-0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  36%|███▌      | 9/25 [00:23<00:40,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  40%|████      | 10/25 [00:25<00:34,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  44%|████▍     | 11/25 [00:27<00:32,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  48%|████▊     | 12/25 [00:29<00:28,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n",
      "\n",
      "--- Running validation at batch 12 ---\n",
      "-1.00\n",
      "-0.50\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "0.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  52%|█████▏    | 13/25 [00:37<00:47,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  56%|█████▌    | 14/25 [00:39<00:35,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  60%|██████    | 15/25 [00:44<00:37,  3.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  64%|██████▍   | 16/25 [00:45<00:26,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  68%|██████▊   | 17/25 [00:50<00:29,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  72%|███████▏  | 18/25 [00:55<00:29,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  76%|███████▌  | 19/25 [00:57<00:21,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  80%|████████  | 20/25 [00:59<00:14,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.75\n",
      "0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  84%|████████▍ | 21/25 [01:01<00:10,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "-0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  88%|████████▊ | 22/25 [01:02<00:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.00\n",
      "-0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  92%|█████████▏| 23/25 [01:04<00:04,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "-0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  96%|█████████▌| 24/25 [01:07<00:02,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "1.00\n",
      "\n",
      "--- Running validation at batch 24 ---\n",
      "-1.00\n",
      "-1.00\n",
      "-0.50\n",
      "-1.00\n",
      "0.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 25/25 [01:19<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "Epoch 1 avg train loss per batch: -2.3968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   4%|▍         | 1/25 [00:01<00:45,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:   8%|▊         | 2/25 [00:05<01:09,  3.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.75\n",
      "-0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  12%|█▏        | 3/25 [00:09<01:12,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  16%|█▌        | 4/25 [00:12<01:10,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.50\n",
      "0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  20%|██        | 5/25 [00:15<01:01,  3.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  24%|██▍       | 6/25 [00:18<00:57,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.30\n",
      "0.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  28%|██▊       | 7/25 [00:21<00:54,  3.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  32%|███▏      | 8/25 [00:24<00:49,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\n",
      "0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  36%|███▌      | 9/25 [00:29<01:00,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55\n",
      "0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  40%|████      | 10/25 [00:34<01:03,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.50\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  44%|████▍     | 11/25 [00:40<01:03,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  48%|████▊     | 12/25 [00:45<01:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n",
      "\n",
      "--- Running validation at batch 12 ---\n",
      "-1.00\n",
      "-1.00\n",
      "-0.50\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  52%|█████▏    | 13/25 [01:14<02:25, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  56%|█████▌    | 14/25 [01:20<01:53, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  60%|██████    | 15/25 [01:25<01:27,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  64%|██████▍   | 16/25 [01:30<01:09,  7.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  68%|██████▊   | 17/25 [01:36<00:55,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  72%|███████▏  | 18/25 [01:41<00:44,  6.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  76%|███████▌  | 19/25 [01:46<00:36,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  80%|████████  | 20/25 [01:51<00:29,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  84%|████████▍ | 21/25 [01:57<00:22,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  88%|████████▊ | 22/25 [02:03<00:17,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  92%|█████████▏| 23/25 [02:09<00:11,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2:  96%|█████████▌| 24/25 [02:15<00:05,  5.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n",
      "\n",
      "--- Running validation at batch 24 ---\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 25/25 [02:43<00:00,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "Epoch 2 avg train loss per batch: -7.8853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   4%|▍         | 1/25 [00:05<02:07,  5.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:   8%|▊         | 2/25 [00:11<02:08,  5.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  12%|█▏        | 3/25 [00:16<02:03,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  16%|█▌        | 4/25 [00:22<01:57,  5.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  20%|██        | 5/25 [00:27<01:50,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.00\n",
      "-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3:  24%|██▍       | 6/25 [00:34<01:48,  5.70s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m\n\u001b[1;32m     32\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     33\u001b[0m     prompt_strs, \n\u001b[1;32m     34\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m\n\u001b[1;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Generate samples\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m gen_ids \u001b[38;5;241m=\u001b[39m \u001b[43msample_no_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMP\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Batch decode and parse\u001b[39;00m\n\u001b[1;32m     47\u001b[0m gen_strs \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(gen_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36msample_no_grad\u001b[0;34m(model, batch_prompt_ids, max_new_tokens, temp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mno_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_no_grad\u001b[39m(model, batch_prompt_ids, max_new_tokens\u001b[38;5;241m=\u001b[39mMAX_NEW_TOKENS, temp\u001b[38;5;241m=\u001b[39mTEMP):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# `batch_prompt_ids` is shape (B, T)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_prompt_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# We return only the newly generated portion\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seq[:, batch_prompt_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m):]\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:2597\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2589\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2590\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2591\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2592\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2593\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2594\u001b[0m     )\n\u001b[1;32m   2596\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2597\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2608\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2609\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2610\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2611\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2612\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2613\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2614\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/generation/utils.py:3560\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3558\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3560\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3562\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3563\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3564\u001b[0m     outputs,\n\u001b[1;32m   3565\u001b[0m     model_kwargs,\n\u001b[1;32m   3566\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3567\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:688\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    683\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    684\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m    685\u001b[0m )\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/utils/generic.py:969\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 969\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    971\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:453\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    451\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[0;32m--> 453\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/modeling_layers.py:48\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:308\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda/envs/224r/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:247\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\u001b[38;5;241m.\u001b[39mview(hidden_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    246\u001b[0m cos, sin \u001b[38;5;241m=\u001b[39m position_embeddings\n\u001b[0;32m--> 247\u001b[0m query_states, key_states \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_pos_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LOG_EVERY = 5\n",
    "BATCH_SIZE = 2\n",
    "train_size = 50\n",
    "val_size = 10\n",
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=f\"train[:{train_size}]\")\n",
    "# Subsample validation/test to 150 examples\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=f\"validation[:{val_size}]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-7)\n",
    "\n",
    "# Pre-process datasets to avoid repeated parsing\n",
    "print(\"Pre-processing datasets...\")\n",
    "train_batches = list(get_batches(train_dataset, parser, BATCH_SIZE))\n",
    "val_batches = list(get_batches(val_dataset, parser, BATCH_SIZE))\n",
    "print(f\"Created {len(train_batches)} training batches, {len(val_batches)} validation batches\")\n",
    "\n",
    "# Reduce validation frequency significantly\n",
    "VALIDATION_FREQUENCY = len(train_batches) // 2  \n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    ##### TRAINING LOOP #####\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_idx, (prompt_strs, raw_qs, correct_answers) in enumerate(tqdm(\n",
    "        train_batches, desc=f\"Training Epoch {epoch + 1}\"\n",
    "    )):\n",
    "        # Tokenize once with proper device placement\n",
    "        prompt_ids = tokenizer(\n",
    "            prompt_strs, \n",
    "            return_tensors=\"pt\",\n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        ).input_ids.to(device)\n",
    "\n",
    "        # Generate samples\n",
    "        gen_ids = sample_no_grad(\n",
    "            model, prompt_ids,\n",
    "            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP\n",
    "        )\n",
    "\n",
    "        # Batch decode and parse\n",
    "        gen_strs = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        parsed_outputs = [parser.parse_llm_output(gen_str) for gen_str in gen_strs]\n",
    "        rationales, pred_answers = zip(*parsed_outputs)\n",
    "\n",
    "        # Compute loss components\n",
    "        logprobs = compute_logprobs(model, prompt_ids, gen_ids, temp=TEMP)\n",
    "        rewards = torch.tensor([\n",
    "            compute_LM_reward(\"gpt4o-mini\", ans, question, rationale)\n",
    "            for ans, question, rationale in zip(correct_answers, raw_qs, rationales)\n",
    "        ], device=device, dtype=torch.float32)\n",
    "        \n",
    "        # REINFORCE loss\n",
    "        loss = -(rewards * logprobs).mean()\n",
    "        # REINFORCE (baseline-normalized) loss\n",
    "        baseline = rewards.mean()\n",
    "        loss = -((rewards - baseline) * logprobs).mean()\n",
    "        variance = rewards.var(unbiased=False) + 1e-8 \n",
    "        #loss = -((rewards - baseline) / variance * logprobs).mean()\n",
    "        # Gradient step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Lightweight logging (reduced frequency)\n",
    "        if batch_idx % LOG_EVERY == 0:\n",
    "            writer.add_scalar(\"BatchLoss/train\", loss.item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgLogProb/train\", logprobs.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            writer.add_scalar(\"BatchAvgReward/train\", rewards.mean().item(), epoch * len(train_batches) + batch_idx)\n",
    "            \n",
    "            # Simplified logging - only save one example per LOG_EVERY batches\n",
    "            log_data = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"question\": raw_qs[0],\n",
    "                \"full_llm_output\": gen_strs[0],\n",
    "                \"rationale\": rationales[0],\n",
    "                \"predicted_answer\": pred_answers[0] if pred_answers[0] else \"None\",\n",
    "                \"correct_answer\": correct_answers[0],\n",
    "                \"reward\": float(rewards[0].item()),\n",
    "                \"logprob\": float(logprobs[0].item()),\n",
    "            }\n",
    "            json_path = os.path.join(LOG_JSON_DIR, \"train\", f\"epoch{epoch + 1}_batch{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "            with open(json_path, \"w\") as f:\n",
    "                json.dump(log_data, f, indent=2)\n",
    "\n",
    "        # Aggressive memory cleanup\n",
    "        del prompt_ids, gen_ids, logprobs, rewards, loss\n",
    "        if batch_idx % 10 == 0:  # Clean cache every 10 batches\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        ### VALIDATION LOGGING ###\n",
    "        if batch_idx > 0 and batch_idx % VALIDATION_FREQUENCY == 0:\n",
    "            print(f\"\\n--- Running validation at batch {batch_idx} ---\")\n",
    "            model.eval()\n",
    "            \n",
    "            val_rewards, val_logprobs, val_losses = [], [], []\n",
    "            collected_examples = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_batch in val_batches:\n",
    "                    val_prompt_strs, val_raw_qs, val_correct_answers = val_batch\n",
    "                    \n",
    "                    val_prompt_ids = tokenizer(\n",
    "                        val_prompt_strs, \n",
    "                        return_tensors=\"pt\",\n",
    "                        padding=True, \n",
    "                        truncation=True, \n",
    "                        max_length=MAX_NEW_TOKENS\n",
    "                    ).input_ids.to(device)\n",
    "\n",
    "                    val_gen_ids = sample_no_grad(model, val_prompt_ids, \n",
    "                                            max_new_tokens=MAX_NEW_TOKENS, temp=TEMP)\n",
    "                    val_gen_strs = tokenizer.batch_decode(val_gen_ids, skip_special_tokens=True)\n",
    "                    \n",
    "                    val_parsed = [parser.parse_llm_output(gen) for gen in val_gen_strs]\n",
    "                    val_rationales, val_pred_answers = zip(*val_parsed)\n",
    "                    \n",
    "                    val_logp = compute_logprobs(model, val_prompt_ids, val_gen_ids, temp=TEMP)\n",
    "                    val_rwds = torch.tensor([\n",
    "                            compute_LM_reward(\"gpt4o-mini\", ans, question, rationale)\n",
    "                        for ans, question, rationale in zip(val_correct_answers, val_raw_qs, val_rationales)\n",
    "                    ], device=device, dtype=torch.float32)\n",
    "\n",
    "                    #val_loss = -(val_rwds * val_logp).mean()\n",
    "                    val_loss = -((val_rwds - val_rwds.mean()) * val_logp).mean()\n",
    "\n",
    "                    # REINFORCE loss\n",
    "                    val_loss = -(val_rwds * val_logp).mean()\n",
    "                    baseline = val_rwds.mean()\n",
    "                    # REINFORCE (baseline-normalized) loss\n",
    "                    #val_loss = -((val_rwds - baseline) * val_logp).mean()\n",
    "                    variance = val_rwds.var(unbiased=False) + 1e-8 \n",
    "                    #val_loss = -((val_rwds - baseline) / variance * val_logp).mean()\n",
    "\n",
    "                    # Collect metrics\n",
    "                    val_rewards.extend(val_rwds.cpu().tolist())\n",
    "                    val_logprobs.extend(val_logp.cpu().tolist())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                    # Collect sample examples (limit to reduce memory)\n",
    "                    for i, (q, r, a, p, rew, lp) in enumerate(zip(\n",
    "                        val_raw_qs[:2], val_rationales[:2], val_correct_answers[:2], \n",
    "                        val_pred_answers[:2], val_rwds[:2], val_logp[:2]\n",
    "                    )):\n",
    "                        collected_examples.append({\n",
    "                            \"question\": q,\n",
    "                            \"rationale\": r,\n",
    "                            \"predicted_answer\": p if p else \"None\",\n",
    "                            \"correct_answer\": a,\n",
    "                            \"reward\": float(rew.item()),\n",
    "                            \"logprob\": float(lp.item())\n",
    "                        })\n",
    "\n",
    "                    # Immediate cleanup\n",
    "                    del val_prompt_ids, val_gen_ids, val_logp, val_rwds, val_loss\n",
    "\n",
    "            # Log validation metrics\n",
    "            avg_val_reward = sum(val_rewards) / len(val_rewards)\n",
    "            avg_val_logprob = sum(val_logprobs) / len(val_logprobs)\n",
    "            avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "            \n",
    "            global_step = epoch * len(train_batches) + batch_idx\n",
    "            writer.add_scalar(\"Eval/AvgReward\", avg_val_reward, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLogProb\", avg_val_logprob, global_step)\n",
    "            writer.add_scalar(\"Eval/AvgLoss\", avg_val_loss, global_step)\n",
    "\n",
    "            # Save validation examples (reduced sample size)\n",
    "            log_sample = random.sample(collected_examples, k=min(3, len(collected_examples)))\n",
    "            log_json = {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"batch\": batch_idx,\n",
    "                \"avg_reward\": avg_val_reward,\n",
    "                \"avg_logprob\": avg_val_logprob,\n",
    "                \"avg_loss\": avg_val_loss,\n",
    "                \"sample_questions\": [ex[\"question\"] for ex in log_sample],\n",
    "                \"sample_rationales\": [ex[\"rationale\"] for ex in log_sample],\n",
    "                \"sample_predicted_answers\": [ex[\"predicted_answer\"] for ex in log_sample],\n",
    "                \"sample_correct_answers\": [ex[\"correct_answer\"] for ex in log_sample],\n",
    "                \"sample_rewards\": [ex[\"reward\"] for ex in log_sample],\n",
    "                \"sample_logprobs\": [ex[\"logprob\"] for ex in log_sample],\n",
    "            }\n",
    "            eval_json_path = os.path.join(LOG_JSON_DIR, \"val\", f\"epoch_{epoch + 1}_batch_{batch_idx}.json\")\n",
    "            os.makedirs(os.path.dirname(eval_json_path), exist_ok=True)\n",
    "            with open(eval_json_path, \"w\") as f:\n",
    "                json.dump(log_json, f, indent=2)\n",
    "            \n",
    "            model.train()\n",
    "            torch.cuda.empty_cache() \n",
    "\n",
    "    avg_loss = train_loss / len(train_batches)\n",
    "    print(f\"Epoch {epoch + 1} avg train loss per batch: {avg_loss:.4f}\")\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch + 1)\n",
    "\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fafeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Q: Where is a bald eagle safe?\n",
      "Answer Choices:\n",
      "(a) pine tree\n",
      "(b) open country\n",
      "(c) in washington\n",
      "(d) wildlife refuge\n",
      "(e) sky\n",
      "A:  (e)\n",
      "Explanation: Bald eagles are found in North America. They are found in the eastern United States, and Canada's Northwest Territories. The answer is (e).\n"
     ]
    }
   ],
   "source": [
    "# Test model generation quality post training\n",
    "model.eval()\n",
    "sample_prompt = \"Q: Where is a bald eagle safe?\\nAnswer Choices:\\n(a) pine tree\\n(b) open country\\n(c) in washington\\n(d) wildlife refuge\\n(e) sky\\nA: \"\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(sample_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"Generated:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afd2f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing datasets...\n",
      "\n",
      "==================================================\n",
      "DEBUGGING GENERATION BEFORE TRAINING\n",
      "==================================================\n",
      "\n",
      "=== DEBUGGING GENERATION ===\n",
      "Temperature: 0.7\n",
      "Max new tokens: 200\n",
      "Input prompt: <|im_start|>system\n",
      "You are an expert at applying commonsense reasoning to answer multiple-choice questions. You will be given a question with multiple answer choices, and you will be tasked with provi...\n",
      "Prompt token length: 305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original generation (temp=0.7):\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Greedy generation:\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Low temp generation (0.1):\n",
      "'icopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt helicopt'\n",
      "\n",
      "Repetition check:\n",
      "Is repetitive: False\n",
      "\n",
      "Current generation parameters:\n",
      "TEMP: 0.7\n",
      "MAX_NEW_TOKENS: 200\n",
      "Model vocab size: 151936\n",
      "Tokenizer vocab size: 151665\n"
     ]
    }
   ],
   "source": [
    "# Debug generation\n",
    "import re\n",
    "\n",
    "def debug_generation(model, tokenizer, prompt_str, max_new_tokens=MAX_NEW_TOKENS, temp=TEMP):\n",
    "    \"\"\"Debug helper to inspect generation step by step\"\"\"\n",
    "    print(f\"\\n=== DEBUGGING GENERATION ===\")\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(f\"Max new tokens: {max_new_tokens}\")\n",
    "    print(f\"Input prompt: {prompt_str[:200]}...\")\n",
    "    \n",
    "    prompt_ids = tokenizer(prompt_str, return_tensors=\"pt\").input_ids.to(device)\n",
    "    print(f\"Prompt token length: {prompt_ids.shape[1]}\")\n",
    "    \n",
    "    # Generate with different settings to compare\n",
    "    with torch.no_grad():\n",
    "        # Original generation\n",
    "        gen_ids = sample_no_grad(model, prompt_ids, max_new_tokens=max_new_tokens, temp=temp)\n",
    "        gen_str = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Alternative: greedy decoding\n",
    "        greedy_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens, \n",
    "                                   do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
    "        greedy_str = tokenizer.decode(greedy_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Alternative: low temperature\n",
    "        low_temp_ids = model.generate(prompt_ids, max_new_tokens=max_new_tokens,\n",
    "                                     do_sample=True, temperature=0.1, \n",
    "                                     pad_token_id=tokenizer.eos_token_id)\n",
    "        low_temp_str = tokenizer.decode(low_temp_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"\\nOriginal generation (temp={temp}):\")\n",
    "    print(repr(gen_str[-500:]))  # Show last 500 chars with escape sequences\n",
    "    print(f\"\\nGreedy generation:\")\n",
    "    print(repr(greedy_str[-500:]))\n",
    "    print(f\"\\nLow temp generation (0.1):\")\n",
    "    print(repr(low_temp_str[-500:]))\n",
    "    \n",
    "    # Check for repetitive patterns\n",
    "    def check_repetition(text):\n",
    "        # Look for repeating character sequences\n",
    "        for length in [2, 3, 4, 5]:\n",
    "            pattern = r'(.{' + str(length) + r'})\\1{10,}'  # Same sequence repeated 10+ times\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                print(f\"Found repetitive {length}-char pattern: '{matches[0]}'\")\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    print(f\"\\nRepetition check:\")\n",
    "    is_repetitive = check_repetition(gen_str)\n",
    "    print(f\"Is repetitive: {is_repetitive}\")\n",
    "    \n",
    "    return gen_str, greedy_str, low_temp_str\n",
    "\n",
    "# Modified training loop with debugging\n",
    "# Subsample training to 5000 examples\n",
    "train_dataset = load_dataset(\"commonsense_qa\", split=\"train[:5000]\")\n",
    "val_dataset = load_dataset(\"commonsense_qa\", split=\"validation[:150]\")\n",
    "\n",
    "writer = SummaryWriter()\n",
    "parser = CommonsenseQAParser(tokenizer)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Pre-process datasets\n",
    "print(\"Pre-processing datasets...\")\n",
    "train_batches = list(get_batches(train_dataset, parser, BATCH_SIZE))\n",
    "val_batches = list(get_batches(val_dataset, parser, BATCH_SIZE))\n",
    "\n",
    "# DEBUG: Test generation before training\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEBUGGING GENERATION BEFORE TRAINING\")\n",
    "print(\"=\"*50)\n",
    "sample_prompt = train_batches[0][0][0]  # First prompt from first batch\n",
    "debug_generation(model, tokenizer, sample_prompt)\n",
    "\n",
    "# Add generation parameter validation\n",
    "print(f\"\\nCurrent generation parameters:\")\n",
    "print(f\"TEMP: {TEMP}\")\n",
    "print(f\"MAX_NEW_TOKENS: {MAX_NEW_TOKENS}\")\n",
    "print(f\"Model vocab size: {model.config.vocab_size}\")\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "224r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
